---
title: "Tidy simulation"
description: |
  Using rowwise to save calculation, estimate power or test size, bootstrap distributions
author:
  - name: Ken Butler
    url: http://ritsokiguess.site/blog
date: 2021-11-14
output:
  distill::distill_article:
    self_contained: false
---


## Packages

```{r}
library(tidyverse)
```

## Introduction

To see what might happen when a process is repeated many times, we can calculate. Or we can physically re-run the process many times, and count up the results: simulation of the process.

I want my simulations here to be reproducible, so I will set the random number seed first:

```{r}
set.seed(457299)
```


## Tossing a coin

Imagine we toss a fair coin 10 times. How likely are we to get 8 or more heads? If you remember the binomial distribution, you can work it out. But if you don't? Make a virtual coin, toss it 10 times, count the number of heads, repeat many times, see how many of those are 8 or greater.

Let's set up our virtual coin first:

```{r}
coin <- c("H", "T")
```

and, since getting a head on one toss doesn't prevent a head on others, ten coin tosses would be a sample of size 10 with replacement from this coin:

```{r}
sample(coin, 10, replace = TRUE)
```

Seven heads this time.

I have a mechanism I use for "tidy simulation":

- set up a dataframe with a column called `sim` to label the simulations
- work `rowwise`
- for each `sim`, do one copy of the thing you'll be doing many times (in this case, simulating 10 coin tosses)
- calculate whatever you want to calculate for each `sim`
- summarize the results


For this problem, the code looks like this:

```{r}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(my_sample = list(sample(coin, 10, replace = TRUE))) %>% 
  mutate(heads = sum(my_sample == "H")) %>% 
  count(heads >= 8)
```

It is probably a good idea to run this one line at a time (to see what it does, and later as you develop your own).

In this case, 54 of the 1000 simulated sets of 10 coin tosses gave at least 8 heads, so our estimate of the probability of getting 8 or more heads in 10 tosses of a fair coin is 0.054.

Some notes about the code:

- I am using 1000 simulations as my "many" repeats of tossing a coin 10 times. A larger number would give a more accurate answer, but would take longer to run.^[I will need to knit the multiple simulations in this blog post before I put it up, so I am sticking with 1000, but you may be more patient than I am.]
- working `rowwise` allows us to treat each row of the dataframe we are building as an independent entity. This makes the coding in the two `mutate`s that follow much easier to follow, because our mental model only has to work one row at a time.
- `my_sample` behaves like *one* sample of 10 coin tosses, though in fact it is a whole column of samples of 10 coin tosses. It is a vector of length 10, so to get it into one cell of our dataframe, we wrap it in `list`, making the whole column a list-column.
- Once again thinking of `my_sample` as a single sample, we then count the number of heads in it. I could use `count`, or `table`, but I don't want to get by samples with no heads or no tails. This way counts 1 for each H in the sample, then adds up the counts.^[My base R heritage sometimes shows through.]
- Finally, count up the number of simulated sets of 10 coin tosses that had 8 or more heads. `count` accepts a logical condition as well as a column. (Behind the scenes it constructs a column of `TRUE` and `FALSE` first, and then counts that.)


In this case, we know the right answer:^[The 7 is because using `lower.tail = FALSE` gives  a probability strictly greater than the first input.]

```{r}
pbinom(7, 10, 0.5, lower.tail = FALSE)
```

Our simulation came out very close to this.

Aside: we can work out how accurate our simulation might be by noting that our 1000 simulations are also like Bernoulli trials: each one gives us 8 or more heads or it doesn't, with unknown probability that is precisely the thing that we are trying to estimate. Thus:

```{r}
binom.test(54, 1000)
```

tells us, with 95% confidence, that the probability of 8 or more heads is between 0.041 and 0.070. To nail it down more precisely, use more than 1000 simulations.


## How long is my longest suit?

In the game of bridge, each player, in two partnerships of 2, receives a hand of 13 cards randomly dealt from the usual deck of 52 cards. There is then an "auction" in which the two partnerships compete for the right to name the trump suit and play the hand. Your partner cannot see your cards, and so in the bidding you have to share information about the strength and suit distribution of your hand using standard methods (you are not allowed to deceive your opponents), so that as a partnership you can decide how many tricks you can win between you.

One of the considerations in the bidding is the length of your longest suit, that is, the suit you hold the most cards in. The longest suit might have only 4 cards (eg. if you have 4 spades and 3 of each of the other suits), but if you are lucky^[That is to say, out-of-this-universe lucky.] you might be dealt a hand with 13 cards all of the same suit and have a longest suit of 13 cards. Evidently something in between those is more likely, but *how* likely?

For a simulation, we need to set up a deck of cards and select 13 cards from it *without* replacement (since you can't draw the same card twice in the same hand). The only thing that matters here is the suits, so we'll set up a deck with only suits and no denominations like Ace or King. (This will make the sampling without replacement look a bit odd.)

```{r}
deck <- c(rep("S", 13), rep("H", 13),
          rep("D", 13), rep("C", 13))
deck
```

and deal ourselves a hand of 13 cards thus:

```{r}
hand <- sample(deck, 13, replace = FALSE)
hand
```

and then count the number of cards in each suit:

```{r}
tab <- table(hand)
tab
```

This time the longest suit has four cards:

```{r}
max(tab)
```

Using `table` is safe here, because we don't care whether there are any suits with no cards in the hand, only about the greatest number of cards in any suit that we have cards in.^[We could use a similar approach to estimate the probability of being dealt a *void*, a suit with no cards in it, but we would have to be more careful counting. Counting the number of different suits represented in the hand and seeing whether it is less than 4 would be one way.]

All of that leads us to this:

```{r}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(hand = list(sample(deck, 13, replace = FALSE))) %>% 
  mutate(suits = list(table(hand))) %>% 
  mutate(longest = max(suits)) %>% 
  count(longest)
```

Note: the hands, and the tables of how many cards a hand has in each suit, are more than single numbers, so they need to be wrapped in `list`.

The most likely longest suit has 5 cards in it, a bit less than half the time. According to this, a  longest suit of 8 cards happens about once in 500 hands, and longer longest suits are even less likely. (To estimate these small probabilities accurately, you need a lot of simulations, like, way more than 1000.)

Aside: the standard way of assessing hand *strength* is via high-card points: 4 for an ace, 3 for a king, 2 for a queen and one for a jack. All the other cards count zero. To simulate the number of points you might get in a hand, build a deck with the points for each card. There are four cards of each rank, and nine ranks that are worth no points:

```{r}
deck <- c(rep(4,4), rep(3,4),
          rep(2,4), rep(1,4), rep(0, 36))
```

The simulation process after that is a lot like before:


```{r}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(hand = list(sample(deck, 13, replace = FALSE))) %>% 
  mutate(points = sum(hand)) -> d
d
```

I stopped it there, partly to show what the dataframe looks like at this point (a hand of 13 point values, and a points total that is the sum of these) and partly because I wanted to do about three things with this, and it made sense to save what we have done thus far.

First, a bar chart of how likely each number of points is:

```{r}
ggplot(d, aes(x = points)) + geom_bar()
```

If you do more simulations, you can check whether the shape is indeed smooth (I'm guessing it is). The average number of points is 10 (there are 40 points in the deck and yours is one of four hands) and the distribution is right-skewed because it is possible, though rather unlikely, to get over 20 points.

In most bidding systems, having 13 points justifies opening the bidding (making the first bid in the auction if everyone has passed on their turn before you). How likely is that?

```{r}
d %>% count(points >= 13)
```

Only about a quarter of the time. 

Having 20 or more points qualifies your hand for an opening bid at the 2-level.^[2 notrumps if you have no long or short suits, 2 clubs if you do, or your hand is stronger than 21 points.] How likely is that?

```{r}
d %>% count(points >= 20)
```

A bit of a rarity, less than a 2% shot.

## Bootstrapping a sampling distribution

To return to the messy world of actual applied statistics: there are a lot of procedures based on an assumption of the right things having normal distributions.^[Because that was easier to develop theory for.] One of the commonest questions is whether we should be using the normal-theory procedure or something else (non-parametric, maybe). Let's take an example. The data [here](http://ritsokiguess.site/datafiles/jays15-home.csv) are information about Toronto Blue Jays baseball games from the early part of the 2015 season:

```{r}
my_url <- "http://ritsokiguess.site/datafiles/jays15-home.csv"
jays <- read_csv(my_url)
jays
```

There is a lot of information here, but we're going to focus on the attendances over near the right side, and in particular, we're interested in the mean attendance over all games of which these are a sample ("early-season Blue Jays games in the years between 2010 and 2019", or something like that). There are, of course, lots of reasons that attendances might vary (opposition, weather, weekend vs. weekday, etc.) that we are going to completely ignore here.

The normal^[Joke. You may laugh.] way to estimate a population mean is to use the confidence interval based on the one-sample $t$-test, but before we jump into that, we should look at a graph of the attendances:

```{r}
ggplot(jays, aes(x = attendance)) + geom_histogram(bins = 6)
```

Well, that doesn't look much like a normal distribution. It's very much skewed to the right. There seem to be two^[A third school would say "do a Bayesian analysis with suitable prior and likelihood model", but that's for another discussion.] schools of thought as to what we should do now:

- we have a large enough sample ($n = 25$) so that we should get enough help from the central limit theorem (also expressed as "the $t$-test is robust to non-normality") and therefore the $t$-procedure should be fine.
- this distribution is a long way from being normal, so there is no way we should use a $t$-procedure, instead using a sign test or signed-rank test,^[There are problems with this, too, that I will go into another time.] inverted to get a confidence interval for the median attendance.

Both of these have an air of handwavery about them. How do we decide between them? Well, let's think about this a little more carefully. When it comes to getting confidence limits, it all depends on the *sampling distribution of the sample mean*. If that is close enough to normal, the $t$-interval is good. But this comes from repeated sampling. You conceptualize it by imagining taking lots of samples from the same population, working out the mean of each sample, and making something like a histogram or normal quantile plot of those. But but --- we only have the one sample we have. How to think about possible sample means we might get?

A way around this is to use the **bootstrap**. The idea is to think of the sample we have as a population (resembling, we hope, the population we want to make inferences about of "all possible attendances") and to take samples from our sample(!) of the same size as the sample we had. If we do this the obvious way (without replacement), we'll get back the original sample we had, every time. So what we do instead is to sample from our sample, but *with* replacement so as to get a different set of values each time, with some values missing and some values repeated. Like this:

```{r}
s <- sample(jays$attendance, replace = TRUE)
sort(s)
```

Sorting the sample reveals that  the first two values and the next three are repeats, so there must be some values from the original sample that are missing. (This is the only reason I sorted them.)

The original data had a mean of

```{r}
jays %>% summarise(mean_att = mean(attendance))
```

but the bootstrap sample has a mean of

```{r}
mean(s)
```

different; if we were to take more bootstrap samples, and find the mean of each one, we would get a sense of the sampling distribution of the sample mean. That is to say, we *simulate* the bootstrapped sampling distribution of the sample mean. Given what we've seen in the other simulations, the structure of the code below ought to come as no surprise:

```{r}
tibble(sim = 1:1000) %>% 
  rowwise() %>% 
  mutate(s = list(sample(jays$attendance, replace = TRUE))) %>% 
  mutate(m = mean(s)) -> d
d
```

In words, set up the 1000 simulations and work rowwise as before, then take (for each row) a bootstrap sample of the attendances, and then take the mean of it. I've saved the resulting dataframe so that we can look at it and then do something else with it. The column `s` containing the samples is a list-column again.

Our question was whether this bootstrapped sampling distribution of the sample mean looked like a normal distribution. To see that, a normal quantile plot is the thing:

```{r}
ggplot(d, aes(sample = m)) + stat_qq() + stat_qq_line()
```

That is very close to a normal distribution, and so in fact the $t$-procedure really is fine and the first school of thought is correct (and now we have *evidence*, no hand-waving required):

```{r}
t.test(jays$attendance)
```

A 95% confidence interval for the mean attendance goes from 20500 to 29600.

Another way to go is to use the bootstrapped sampling distribution directly, entirely bypassing all the normal theory, and just take the middle 95% of it:

```{r}
d %>% ungroup() %>% summarize(ci = quantile(m, c(0.025, 0.975)))
```

21000 to 29700, not that different (given the large amount of variability) from the $t$-interval. There are better ways to get the interval rather than using sample quantiles; see for example [here](https://acclab.github.io/bootstrap-confidence-intervals.html). But this will do for now.

The `ungroup` in the code is there because the dataframe `d` is still `rowwise`: everything we do with `d` will still be done one row at a time. But now we want to work on the whole column `m`, so we have to undo the `rowwise` first. `rowwise` is a special case of `group_by` (a sort of group-by-rows), so you undo `rowwise` in the same way that you undo `group_by`.

## Power by simulation

