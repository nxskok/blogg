[
  {
    "path": "posts/2021-11-07-welcome/",
    "title": "Welcome",
    "description": "The new home of my blog",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\nWelcome to the new home for my blog. I have decided to move from blogdown to distill, since it seems to be easier to maintain this way. There will eventually be links to at least some of the old posts, and, I hope, some new ones.\nThe old blog is still up (and will remain so) here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-07T17:56:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-density-plots/",
    "title": "Density plots",
    "description": "An alternative to histograms and boxplots",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2021-10-16",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nSome data\nWe will need some data to illustrate the plots. I will use some data on physical and physiological measurements on 202 Australian elite athletes:\n\n\nmy_url <- \"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes <- read_tsv(my_url)\nathletes\n\n\n# A tibble: 202 × 13\n   Sex   Sport   RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM\n   <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl>\n 1 fema… Netb…  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1\n 2 fema… Netb…  4.15   6    38    12.7    59  21.2 110.     25.3  47.1\n 3 fema… Netb…  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4\n 4 fema… Netb…  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8\n 5 fema… Netb…  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0\n 6 fema… Netb…  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4\n 7 fema… Netb…  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1\n 8 fema… Netb…  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4\n 9 fema… Netb…  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0\n10 fema… Netb…  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6\n# … with 192 more rows, and 2 more variables: Ht <dbl>, Wt <dbl>\n\nThe histogram and the boxplot\nThe histogram goes back to Karl Pearson. It offers a simple way to visualize a single quantitative, continuous distribution. For example, a histogram of the weights of all the athletes:\n\n\nggplot(athletes, aes(x = Wt)) + geom_histogram(bins = 8)\n\n\n\n\nA histogram works by dividing the range of the quantitative variable into discrete intervals or “bins”, often of the same width. On the above histogram, the tallest bar goes with a bin from about 65 to about 80 (kg). The height of each histogram bar is the number of observations within that bin.\nEvidently, the choice of how many bins to use may have an impact on how the histogram looks. If you use more bins:\n\n\nggplot(athletes, aes(x = Wt)) + geom_histogram(bins = 20)\n\n\n\n\nyou get a more detailed picture of the distribution, at the expense of getting a less smooth picture. In this picture, the distribution appears to have something like three peaks, but you would probably say that these only appeared because we chose so many bins. On the other hand, if you have fewer bins:\n\n\nggplot(athletes, aes(x = Wt)) + geom_histogram(bins = 3)\n\n\n\n\nyou get a smooth picture all right, but you lose almost all of the detail.\nHadley Wickham, in “R For Data Science”, says “You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns.” This may be the reason that there is no default number of bins on geom_histogram (well, there is, but the default is way too many). The right number of bins for you and your histogram depends on the story you think the data are trying to tell.\nA boxplot is another way to make a visual of a quantitative variable. The boxplot was popularized by John Tukey, but is really a descendant of the “range bar” of Mary Eleanor Spear. Here’s how it looks for the weights:\n\n\nggplot(athletes, aes(x = 1, y = Wt)) + geom_boxplot()\n\n\n\n\nA boxplot is characterized by a box, two whiskers and some observations plotted individually. The line across the middle of the box is at the median; the bottom and top of the box are the first and third quartiles, so that the height of the box is the interquartile range; the whiskers join the box to the outermost “non-extreme” observations, and any “extreme” observations are plotted individually. (The usual criterion for “extreme” is more than 1.5 times the inter-quartile range beyond the quartiles.)\nWhere a boxplot really shines is in comparing several groups, for example, comparing the weights of athletes who play different sports:\n\n\nggplot(athletes, aes(x = Sport, y = Wt)) + geom_boxplot()\n\n\n\n\nThe gymnasts (Gym) are the least heavy on average, and also have a very small spread. (In this dataset, the gymnasts are all female.) The field athletes (Field) and the water polo players (WPolo) are the heaviest on average. There are some outliers (the heavy track sprinter (TSprnt) and the light netball player), and the rowers’ (Row) distribution has a long lower tail (the low extreme point may be an outlier or part of the long tail).\nThe value of the histogram and the boxplot is that they are easy to draw by hand. For example, you could first draw a stem-and-leaf plot of the data, and use the information there to count the number of observations in bins, or to work out the quartiles and find the extreme observations. (Many of Tukey’s other innovations were designed to be quick and simple, readily usable on a 1960s factory floor.) This is good if you were learning or using statistical methods in the days before computers, or you are pretending you are (not mentioning stats classes in Psychology at all, oh dear me no). But with access to R, there is no need to restrict ourselves to graphs that are easy to draw.\nDensity estimation\nThe histogram is a rather crude example of a “density estimator”: the number of observations per unit interval of (in our case) kilogram of weight. A basic way of estimating density at a certain weight, say 80 kg, is to take the number of observations in the histogram bin that includes 80, and then to divide it by the width of the bin.\nA limitation of the above method is that the estimate is the same all the way across the bin, giving a discontinuous estimate of density when the underlying density curve ought (you would expect) to be smooth.\nSomething that will give a smooth answer is kernel density estimation. For any given (athlete’s) weight \\(x\\), we choose a distribution centred at \\(x\\) (say normal) and choose a standard deviation for that distribution (there are rules of thumb for doing this). This is called a kernel. To estimate the density at \\(x\\), work out a weighted count of how many observations there are close to \\(x\\), where the weights are the kernel distribution’s density function evaluated at each observation. The idea is that observations closer to \\(x\\) should contribute more to the weighted count.\nThis is not something you would want to calculate by hand, but we are no longer in the 1960s, so we no longer need to do that. Here is the kernel-density-estimated smoothed histogram for the athletes’ weights:\n\n\nggplot(athletes, aes(x = Wt)) + geom_density()\n\n\n\n\nA nice smooth version of the histogram. By default, this uses a normal kernel distribution with a standard deviation chosen by a rule of thumb. The smoothness can be adjusted by using a value of adjust different from 1:\n\n\nggplot(athletes, aes(x = Wt)) + geom_density(adjust = 3)\n\n\n\n\nA value bigger than 1 estimates from a wider range of data, so the resulting density plot looks smoother.\n\n\nggplot(athletes, aes(x = Wt)) + geom_density(adjust = 0.3)\n\n\n\n\nA value of adjust less than 1 reacts more sharply to local features of the data, producing a less smooth graph.\nAnother feature of density plots is that you can “stack” several behind each other, to compare distributions. Let’s compare the weights of the male and female1 athletes. To do that, use a fill on the aes and specify the categorical variable there:\n\n\nggplot(athletes, aes(x = Wt, fill = Sex)) + geom_density()\n\n\n\n\nThis is not quite as we want: the upper tail of the weight distribution for the female athletes has disappeared behind the male ones, so we don’t know how high the female athletes’ weights go. To make it so that we can see both, we need to make the density curves partly transparent. In ggplot, you can make anything transparent by using a parameter alpha; a value of 1 means completely opaque (ie. like this), and a value of 0 means completely transparent (ie. invisible). So we can try this:\n\n\nggplot(athletes, aes(x = Wt, fill = Sex)) + geom_density(alpha = 0.5)\n\n\n\n\nThis makes it clearer that both distributions of weights have an upper tail, and that the female athletes’ weights to up to about 100 kg, heavier than the average of the male athlete weights. The athletes that are lighter in weight are all females, however.\nI’d have to say, though, that comparing distributions using density plots is easier with a relatively small number of distributions. Comparing all ten sports might be too much:\n\n\nggplot(athletes, aes(x = Wt, fill = Sport)) + geom_density(alpha = 0.5)\n\n\n\n\nIt’s more than a little difficult to distinguish all those colours, never mind to see where their density estimates are. When using fill, which colours the inside of something in ggplot, the colours of overlapping things also get mixed. An alternative is to use colour rather than fill, which colours the outside:\n\n\nggplot(athletes, aes(x = Wt, colour = Sport)) + geom_density(alpha = 0.5)\n\n\n\n\nThis time, it’s a little easier to see where each density goes, but it is still equally difficult to distinguish the ten colours from each other. For this many distributions, the boxplot is still a good way to compare them.\nIf we just look at, say, four of the sports, the density plot is more useful:\n\n\nthe_sports <- c(\"Gym\", \"Netball\", \"BBall\", \"Field\")\nathletes %>% \n  filter(Sport %in% the_sports) %>% \n  ggplot(aes(x = Wt, fill = Sport)) + geom_density(alpha = 0.5)\n\n\n\n\nThis shows that the gymnasts are the lightest weight, the netball players have a compact distribution of weights centred around 65 kg, and the basketball players and field athletes have a much greater spread of weights, with the field athletes being heavier overall. (It’s a little difficult to see the distribution of weights of basketball players other than by elimination, because the density estimate for basketball players is hidden behind the others except around weight 80 kg.)\nFinal thoughts\nFor a less statistically-educated audience, the density plot has less “baggage” than other plots like the boxplot; it is rather clearer where most of the values are, whether there is a greater or lesser spread, and what the shape looks like. It is also straightforward to see how distributions compare by making a density plot like the one above with the density estimates one behind the other. For a statistical audience, it is clear by looking at a boxplot how distributions compare, but boxplots have more baggage in that for a general audience, there needs to be discussion of median, quartiles and outliers in order to make sense of the plot. I was motivated to write this post because I did a “webinar” for my Toastmasters club using the Palmer Penguins data, and in preparing that, I found that density plots gave a clear picture of how the three species of penguin compared on the physical measurements in that dataset.\n\nFor this dataset, that means eligible to compete in athletic events for men and women.↩︎\n",
    "preview": "posts/2021-11-07-density-plots/density-plots_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2021-11-07T20:13:30-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-07-correcting-a-dataframe/",
    "title": "Correcting a dataframe",
    "description": "The tidyverse way.",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2021-04-26",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\nlibrary(tmaptools)\nlibrary(leaflet)\n\n\n\nIntroduction\nSo I had a dataframe today, in which I wanted to make some small corrections. Specifically, I had this one:\n\n\nmy_url <- \"http://ritsokiguess.site/datafiles/wisconsin.txt\"\nwisc <- read_table(my_url)\nwisc %>% select(location)\n\n\n# A tibble: 12 × 1\n   location     \n   <chr>        \n 1 Appleton     \n 2 Beloit       \n 3 Fort.Atkinson\n 4 Madison      \n 5 Marshfield   \n 6 Milwaukee    \n 7 Monroe       \n 8 Superior     \n 9 Wausau       \n10 Dubuque      \n11 St.Paul      \n12 Chicago      \n\nThese are mostly, but not all, cities in Wisconsin, and I want to draw them on a map. To do that, though, I need to affix their states to them, and I thought a good starting point was to start by pretending that they were all in Wisconsin, and then correct the ones that aren’t:\n\n\nwisc %>% select(location) %>% \n  mutate(state = \"WI\") -> wisc\nwisc\n\n\n# A tibble: 12 × 2\n   location      state\n   <chr>         <chr>\n 1 Appleton      WI   \n 2 Beloit        WI   \n 3 Fort.Atkinson WI   \n 4 Madison       WI   \n 5 Marshfield    WI   \n 6 Milwaukee     WI   \n 7 Monroe        WI   \n 8 Superior      WI   \n 9 Wausau        WI   \n10 Dubuque       WI   \n11 St.Paul       WI   \n12 Chicago       WI   \n\nThe last three cities are in the wrong state: Dubuque is in Iowa (IA), St. Paul in Minnesota (MN), and Chicago is in Illinois (IL). I know how to fix this in base R: I write something like\n\n\nwisc$state[12] <- \"IL\"\n\n\n\nbut how do you do this the Tidyverse way?\nA better way\nThe first step is to make a small dataframe with the cities that need to be corrected, and the states they are actually in:\n\n\ncorrections <- tribble(\n  ~location, ~state,\n  \"Dubuque\", \"IA\",\n  \"St.Paul\", \"MN\",\n  \"Chicago\", \"IL\"\n)\ncorrections\n\n\n# A tibble: 3 × 2\n  location state\n  <chr>    <chr>\n1 Dubuque  IA   \n2 St.Paul  MN   \n3 Chicago  IL   \n\nNote that the columns of this dataframe have the same names as the ones in the original dataframe wisc.\nSo, I was thinking, this is a lookup table (of a sort), and so joining this to wisc might yield something helpful. We want to look up locations and not match states, since we want to have these three cities have their correct state as a possibility. So what does this do?\n\n\nwisc %>% \n  left_join(corrections, by = \"location\")\n\n\n# A tibble: 12 × 3\n   location      state.x state.y\n   <chr>         <chr>   <chr>  \n 1 Appleton      WI      <NA>   \n 2 Beloit        WI      <NA>   \n 3 Fort.Atkinson WI      <NA>   \n 4 Madison       WI      <NA>   \n 5 Marshfield    WI      <NA>   \n 6 Milwaukee     WI      <NA>   \n 7 Monroe        WI      <NA>   \n 8 Superior      WI      <NA>   \n 9 Wausau        WI      <NA>   \n10 Dubuque       WI      IA     \n11 St.Paul       WI      MN     \n12 Chicago       WI      IL     \n\nNow, we have two states for each city. The first one is always Wisconsin, and the second one is usually missing, but where the state in state.y has a value, that is the true state of the city. So, the thought process is that the actual state should be:\nif state.y is not missing, use that\nelse, use the value in state.x.\nI had an idea that there was a function that would do exactly this, only I couldn’t remember its name, so I couldn’t really search for it. My first thought was na_if. What this does is every time it sees a certain value, it replaces it with NA. This, though, is the opposite way from what I wanted. So I looked at the See Also, and saw replace_na. This replaces NAs with a given value. Not quite right, but closer.\nIn the See Also for replace_na, I saw one more thing: coalesce, “replace NAs with values from other vectors”. Was that what I was thinking of? It was. The way it works is that you feed it several vectors, and the first one that is not missing gives its value to the result. Hence, what I needed was this:\n\n\nwisc %>% \n  left_join(corrections, by = \"location\") %>% \n  mutate(state=coalesce(state.y, state.x))\n\n\n# A tibble: 12 × 4\n   location      state.x state.y state\n   <chr>         <chr>   <chr>   <chr>\n 1 Appleton      WI      <NA>    WI   \n 2 Beloit        WI      <NA>    WI   \n 3 Fort.Atkinson WI      <NA>    WI   \n 4 Madison       WI      <NA>    WI   \n 5 Marshfield    WI      <NA>    WI   \n 6 Milwaukee     WI      <NA>    WI   \n 7 Monroe        WI      <NA>    WI   \n 8 Superior      WI      <NA>    WI   \n 9 Wausau        WI      <NA>    WI   \n10 Dubuque       WI      IA      IA   \n11 St.Paul       WI      MN      MN   \n12 Chicago       WI      IL      IL   \n\nWhere state.y has a value, it is used; if it’s missing, the value in state.x is used instead.\nThe best way\nI was quite pleased with myself for coming up with this, but I had missed the actual best way of doing this. In SQL, there is UPDATE, and what that does is to take a table of keys to look up and some new values for other columns to replace the ones in the original table. Because dplyr has a lot of things in common with SQL, it is perhaps no surprise that there is a rows_update, and for this job it is as simple as this:\n\n\nwisc %>% \n  rows_update(corrections) -> wisc\nwisc\n\n\n# A tibble: 12 × 2\n   location      state\n   <chr>         <chr>\n 1 Appleton      WI   \n 2 Beloit        WI   \n 3 Fort.Atkinson WI   \n 4 Madison       WI   \n 5 Marshfield    WI   \n 6 Milwaukee     WI   \n 7 Monroe        WI   \n 8 Superior      WI   \n 9 Wausau        WI   \n10 Dubuque       IA   \n11 St.Paul       MN   \n12 Chicago       IL   \n\nThe values to look up (the “keys”) are by default in the first column, which is where they are in corrections. If they had not been, I would have used a by in the same way as with a join.\nMind. Blown. (Well, my mind was, anyway.)\nGeocoding\nI said I wanted to draw a map with these cities on it. For that, I need to look up the longitude and latitude of these places, and for that, I need to glue the state onto the name of each city, to make sure I don’t look up the wrong one. It is perhaps easy to forget that unite is the cleanest way of doing this, particularly if you don’t want the individual columns any more:\n\n\nwisc %>% unite(where, c(location, state), sep = \" \") -> wisc\nwisc\n\n\n# A tibble: 12 × 1\n   where           \n   <chr>           \n 1 Appleton WI     \n 2 Beloit WI       \n 3 Fort.Atkinson WI\n 4 Madison WI      \n 5 Marshfield WI   \n 6 Milwaukee WI    \n 7 Monroe WI       \n 8 Superior WI     \n 9 Wausau WI       \n10 Dubuque IA      \n11 St.Paul MN      \n12 Chicago IL      \n\nThe function geocode_OSM from tmaptools will find the longitude and latitude of a place. It expects one place as input, not a vector of placenames, so we will work rowwise to geocode one at a time. (Using map from purrr is also an option.) The geocoder returns a list, which contains, buried a little deeply, the longitudes and latitudes:\n\n\nwisc %>% \n  rowwise() %>% \n  mutate(ll = list(geocode_OSM(where))) -> wisc\nwisc\n\n\n# A tibble: 12 × 2\n# Rowwise: \n   where            ll              \n   <chr>            <list>          \n 1 Appleton WI      <named list [3]>\n 2 Beloit WI        <named list [3]>\n 3 Fort.Atkinson WI <named list [3]>\n 4 Madison WI       <named list [3]>\n 5 Marshfield WI    <named list [3]>\n 6 Milwaukee WI     <named list [3]>\n 7 Monroe WI        <named list [3]>\n 8 Superior WI      <named list [3]>\n 9 Wausau WI        <named list [3]>\n10 Dubuque IA       <named list [3]>\n11 St.Paul MN       <named list [3]>\n12 Chicago IL       <named list [3]>\n\nThe column ll is a list-column, and the usual way to handle these is to unnest, but that isn’t quite right here:\n\n\nwisc %>% unnest(ll)\n\n\n# A tibble: 36 × 2\n   where            ll          \n   <chr>            <named list>\n 1 Appleton WI      <chr [1]>   \n 2 Appleton WI      <dbl [2]>   \n 3 Appleton WI      <bbox [4]>  \n 4 Beloit WI        <chr [1]>   \n 5 Beloit WI        <dbl [2]>   \n 6 Beloit WI        <bbox [4]>  \n 7 Fort.Atkinson WI <chr [1]>   \n 8 Fort.Atkinson WI <dbl [2]>   \n 9 Fort.Atkinson WI <bbox [4]>  \n10 Madison WI       <chr [1]>   \n# … with 26 more rows\n\nUnnesting a list of three things produces three rows for each city. It would make more sense to have the unnesting go to the right and produce a new column for each thing in the list. The new tidyr has a variant called unnest_wider that does this:\n\n\nwisc %>% \n  unnest_wider(ll)\n\n\n# A tibble: 12 × 4\n   where            query            coords    bbox      \n   <chr>            <chr>            <list>    <list>    \n 1 Appleton WI      Appleton WI      <dbl [2]> <bbox [4]>\n 2 Beloit WI        Beloit WI        <dbl [2]> <bbox [4]>\n 3 Fort.Atkinson WI Fort.Atkinson WI <dbl [2]> <bbox [4]>\n 4 Madison WI       Madison WI       <dbl [2]> <bbox [4]>\n 5 Marshfield WI    Marshfield WI    <dbl [2]> <bbox [4]>\n 6 Milwaukee WI     Milwaukee WI     <dbl [2]> <bbox [4]>\n 7 Monroe WI        Monroe WI        <dbl [2]> <bbox [4]>\n 8 Superior WI      Superior WI      <dbl [2]> <bbox [4]>\n 9 Wausau WI        Wausau WI        <dbl [2]> <bbox [4]>\n10 Dubuque IA       Dubuque IA       <dbl [2]> <bbox [4]>\n11 St.Paul MN       St.Paul MN       <dbl [2]> <bbox [4]>\n12 Chicago IL       Chicago IL       <dbl [2]> <bbox [4]>\n\nThe longitudes and latitudes we want are still hidden in a list-column, the one called coords, so with luck, if we unnest that wider as well, we should be in business:\n\n\nwisc %>% \n  unnest_wider(ll) %>% \n  unnest_wider(coords) -> wisc\nwisc\n\n\n# A tibble: 12 × 5\n   where            query                x     y bbox      \n   <chr>            <chr>            <dbl> <dbl> <list>    \n 1 Appleton WI      Appleton WI      -88.4  44.3 <bbox [4]>\n 2 Beloit WI        Beloit WI        -89.0  42.5 <bbox [4]>\n 3 Fort.Atkinson WI Fort.Atkinson WI -88.8  42.9 <bbox [4]>\n 4 Madison WI       Madison WI       -89.4  43.1 <bbox [4]>\n 5 Marshfield WI    Marshfield WI    -90.2  44.7 <bbox [4]>\n 6 Milwaukee WI     Milwaukee WI     -87.9  43.0 <bbox [4]>\n 7 Monroe WI        Monroe WI        -89.6  42.6 <bbox [4]>\n 8 Superior WI      Superior WI      -92.1  46.6 <bbox [4]>\n 9 Wausau WI        Wausau WI        -89.6  45.0 <bbox [4]>\n10 Dubuque IA       Dubuque IA       -90.7  42.5 <bbox [4]>\n11 St.Paul MN       St.Paul MN       -93.1  44.9 <bbox [4]>\n12 Chicago IL       Chicago IL       -87.6  41.9 <bbox [4]>\n\nAnd now we are. x contains the longitudes (negative for degrees west), and y the latitudes (positive for degrees north).\nMaking a map with these on them\nThe most enjoyable way to make a map in R is to use the leaflet package. Making a map is a three-step process:\nleaflet() with the name of the dataframe\naddTiles() to get map tiles to draw the map with\nadd some kind of markers to show where the points are. I use circle markers here; there are also markers (from addMarkers) that look like Google map pins. Here also you associate the longs and lats with the columns they are in in your dataframe:\n\n\nleaflet(data = wisc) %>% \n  addTiles() %>% \n  addCircleMarkers(lng = ~x, lat = ~y) \n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircleMarkers\",\"args\":[[44.2613967,42.5083272,42.9288944,43.074761,44.6688524,43.0349931,42.6018298,46.623324,44.9596017,42.5006217,44.9497487,41.8755616],[-88.4069744,-89.031784,-88.8370509,-89.3837613,-90.1717987,-87.922497,-89.6392396,-92.117435,-89.6298239,-90.6647967,-93.0931028,-87.6244212],10,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2},null,null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[41.8755616,46.623324],\"lng\":[-93.0931028,-87.6244212]}},\"evals\":[],\"jsHooks\":[]}\nThe nice thing about Leaflet maps is that you can zoom, pan and generally move about in them. For example, you can zoom in to find out which city each circle represents.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-07T21:32:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-sampling-locations-in-a-city/",
    "title": "Sampling locations in a city",
    "description": "with the aim of getting an aerial map of that location.",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2020-10-10",
    "categories": [],
    "contents": "\nIntroduction\nDo you follow @londonmapbot on Twitter? You should. Every so often a satellite photo is posted of somewhere in London (the one in England), with the implied invitation to guess where it is. Along with the tweet is a link to openstreetmap, and if you click on it, it gives you a map of where the photo is, so you can see whether your guess was right. Or, if you’re me, you look at the latitude and longitude in the link, and figure out roughly where in the city it is. My strategy is to note that Oxford Circus, in the centre of London, is at about 51.5 north and 0.15 west, and work from there.1\nMatt Dray, who is behind @londonmapbot, selects random points in a rectangle that goes as far in each compass direction as the M25 goes. (This motorway surrounds London in something like a circle, and is often taken as a definition of what is considered to be London; if outside, not in London. There is a surprising amount of countryside inside the M25.)\nLondon has the advantage of being roughly a rectangle aligned north-south and east-west, and is therefore easy to sample from. I have been thinking about doing something similar for my home city Toronto, but I ran into an immediate problem:\nToronto with boundaryToronto is not nicely aligned north-south and east-west, and so if you sample from a rectangle enclosing it, this is what will happen:\nrandomly sampled points from rectangle surrounding TorontoYou get some points inside the city, but you will also get a number of points in Vaughan or Mississauga or Pickering or Lake Ontario! How to eliminate the ones I don’t want?\nSampling from a region\nLet’s load some packages:\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sp)\n\n\n\nI had this vague idea that it would be possible to decide if a sampled point was inside a polygon or not. So I figured I would start by defining the boundary of Toronto as a collection of straight lines joining points, at least approximately. The northern boundary of Toronto is Steeles Avenue, all the way across, and that is a straight line, but the southern boundary is Lake Ontario, and the western and eastern boundaries are a mixture of streets and rivers, so I tried to pick points which, when joined by straight lines, enclosed all of Toronto without too much extra. This is what I came up with:\n\n\nboundary <- tribble(\n  ~where, ~lat, ~long,\n\"Steeles @ 427\", 43.75, -79.639,\n\"Steeles @ Pickering Townline\", 43.855, -79.17,\n\"Twyn Rivers @ Rouge River\", 43.815, -79.15,\n\"Rouge Beach\", 43.795, -79.115,\n\"Tommy Thompson Park\", 43.61, -79.33,\n\"Gibraltar Point\", 43.61, -79.39,\n\"Sunnyside Beach\", 43.635, -79.45,\n\"Cliff Lumsden Park\", 43.59, -79.50,\n\"Marie Curtis Park\", 43.58, -79.54,\n\"Rathburn @ Mill\", 43.645, -79.59,\n\"Etobicoke Creek @ Eglinton\", 43.645, -79.61,\n\"Eglinton @ Renforth\", 43.665, -79.59,\n\"Steeles @ 427\", 43.75, -79.639,\n)\nboundary\n\n\n# A tibble: 13 × 3\n   where                          lat  long\n   <chr>                        <dbl> <dbl>\n 1 Steeles @ 427                 43.8 -79.6\n 2 Steeles @ Pickering Townline  43.9 -79.2\n 3 Twyn Rivers @ Rouge River     43.8 -79.2\n 4 Rouge Beach                   43.8 -79.1\n 5 Tommy Thompson Park           43.6 -79.3\n 6 Gibraltar Point               43.6 -79.4\n 7 Sunnyside Beach               43.6 -79.4\n 8 Cliff Lumsden Park            43.6 -79.5\n 9 Marie Curtis Park             43.6 -79.5\n10 Rathburn @ Mill               43.6 -79.6\n11 Etobicoke Creek @ Eglinton    43.6 -79.6\n12 Eglinton @ Renforth           43.7 -79.6\n13 Steeles @ 427                 43.8 -79.6\n\nI kind of had the idea that you could determine whether a point was inside a polygon or not. The idea turns out to be this: you draw a line to the right from your point; if it crosses the boundary of the polygon an odd number of times, it’s inside, and if an even number of times, it’s outside. So is there something like this in R? Yes: this function in the sp package.2\nSo now I could generate some points in the enclosing rectangle and see whether they were inside or outside the city, like this:\n\n\nset.seed(457299)\nn_point <- 20\ntibble(lat = runif(n_point, min(boundary$lat), max(boundary$lat)),\n       long = runif(n_point, min(boundary$long), max(boundary$long))) -> d\nd %>% mutate(inside = point.in.polygon(d$long, d$lat, boundary$long, boundary$lat)) %>% \n  mutate(colour = ifelse(inside == 1, \"blue\", \"red\")) -> d\nd\n\n\n# A tibble: 20 × 4\n     lat  long inside colour\n   <dbl> <dbl>  <int> <chr> \n 1  43.8 -79.6      0 red   \n 2  43.6 -79.5      0 red   \n 3  43.6 -79.6      1 blue  \n 4  43.7 -79.3      1 blue  \n 5  43.7 -79.2      0 red   \n 6  43.8 -79.3      1 blue  \n 7  43.6 -79.2      0 red   \n 8  43.6 -79.2      0 red   \n 9  43.7 -79.5      1 blue  \n10  43.8 -79.2      1 blue  \n11  43.8 -79.4      1 blue  \n12  43.7 -79.5      1 blue  \n13  43.6 -79.1      0 red   \n14  43.7 -79.4      1 blue  \n15  43.8 -79.4      1 blue  \n16  43.8 -79.2      1 blue  \n17  43.7 -79.3      1 blue  \n18  43.7 -79.1      0 red   \n19  43.8 -79.6      0 red   \n20  43.7 -79.2      1 blue  \n\nThe function point.in.polygon returns a 1 if the point is inside the polygon (city boundary) and a 0 if outside.3\nI added a column colour to plot the inside and outside points in different colours on a map, which we do next. The leaflet package is much the easiest way to do this:\n\n\nleaflet(d) %>% \n  addTiles() %>% \n  addCircleMarkers(color = d$colour) %>% \n    addPolygons(boundary$long, boundary$lat)\n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircleMarkers\",\"args\":[[43.8405856347538,43.6142636974005,43.6426254201052,43.7333733423392,43.6883476533822,43.7869079166744,43.6465799339535,43.6396968865662,43.740718059208,43.8079513767513,43.7892276994954,43.6821760909643,43.6186219538923,43.7004526383779,43.7960451051954,43.7757609975478,43.7179439867509,43.6734056267957,43.8174444213754,43.7353308267088],[-79.6133690229235,-79.4667217004932,-79.5794000967983,-79.3068632841418,-79.1685870058518,-79.3238925372232,-79.2202741911337,-79.2137170310309,-79.5196319680633,-79.2255765117444,-79.3659771945663,-79.5367257098705,-79.1490086372094,-79.4227933171159,-79.4004283973714,-79.2079592188233,-79.321934687146,-79.1151048495993,-79.6166347121717,-79.1885206499314],10,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":[\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\"],\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":[\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\"],\"fillOpacity\":0.2},null,null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addPolygons\",\"args\":[[[[{\"lng\":[-79.639,-79.17,-79.15,-79.115,-79.33,-79.39,-79.45,-79.5,-79.54,-79.59,-79.61,-79.59,-79.639],\"lat\":[43.75,43.855,43.815,43.795,43.61,43.61,43.635,43.59,43.58,43.645,43.645,43.665,43.75]}]]],null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2,\"smoothFactor\":1,\"noClip\":false},null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[43.58,43.855],\"lng\":[-79.639,-79.115]}},\"evals\":[],\"jsHooks\":[]}\nThe polygons come from a different dataframe, so I need to specify that in addPolygons. Leaflet is clever enough to figure out which is longitude and which latitude (there are several possibilities it will understand).\nThis one seems to have classified the points more or less correctly. The bottom left red circle is just in the lake, though it looks as if one of the three rightmost blue circles is in the lake also. Oops. The way to test this is to generate several sets of random points, test the ones near the boundary, and if they were classified wrongly, tweak the boundary points and try again. The coastline around the Scarborough Bluffs is not as straight as I was hoping.\nMapbox\nMatt Dray’s blog post gives a nice clear explanation of how to set up MapBox to return you a satellite image of a lat and long you feed it. What you need is a Mapbox API key. A good place to save this is in your .Renviron, and edit_r_environ from usethis is a good way to get at that. Then you use this key to construct a URL that will return you an image of that point.\nLet’s grab one of those sampled points that actually is in Toronto:\n\n\nd %>% filter(inside == 1) %>% slice(1) -> d1\nd1\n\n\n# A tibble: 1 × 4\n    lat  long inside colour\n  <dbl> <dbl>  <int> <chr> \n1  43.6 -79.6      1 blue  \n\nand then I get my API key and use it to make a URL for an image at this point:\n\n\nmapbox_token <- Sys.getenv(\"MAPBOX_TOKEN\")\nurl <- str_c(\"https://api.mapbox.com/styles/v1/mapbox/satellite-v9/static/\",\n             d1$long,\n             \",\",\n             d1$lat,\n             \",15,0/600x400?access_token=\",\n             mapbox_token)\n\n\n\nI’m not showing you the actual URL, since it contains my key! The last-but-one line contains the zoom (15) and the size of the image (600 by 400). These are slightly more zoomed out and bigger than the values Matt uses. (I wanted to have a wider area to make it easier to guess.)\nThen download this and save it somewhere:\n\n\nwhere <- \"img.png\"\ndownload.file(url, where)\n\n\n\nand display it:\nsatellite image of somewhere in TorontoI don’t recognize that, so I’ll fire up leaflet again:\n\n\nleaflet(d1) %>% \n  addTiles() %>% \n  addCircleMarkers() \n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircleMarkers\",\"args\":[43.6426254201052,-79.5794000967983,10,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2},null,null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[43.6426254201052,43.6426254201052],\"lng\":[-79.5794000967983,-79.5794000967983]}},\"evals\":[],\"jsHooks\":[]}\nIt’s the bit of Toronto that’s almost in Mississauga. The boundary is Etobicoke Creek, at the bottom left of the image.\nReferences\nHow to determine if point inside polygon\npoint.in.polygon function documentation\nMatt Dray blog post on londonmapbot\n\nLondon extends roughly between latitude 51.2 and 51.7 degrees, and between longitude 0.25 degrees east and 0.5 west. Knowing this enables you to place a location in London from its lat and long.↩︎\nHaving had a bad experience with rgdal earlier, I was afraid that sp would be a pain to install, but there was no problem at all.↩︎\nIt also returns a 2 if the point is on an edge of the polygon and a 3 if at a vertex.↩︎\n",
    "preview": "posts/2021-11-07-sampling-locations-in-a-city/Screenshot_2020-10-10_12-40-39.png",
    "last_modified": "2021-11-07T21:16:35-05:00",
    "input_file": {},
    "preview_width": 617,
    "preview_height": 431
  },
  {
    "path": "posts/2020-07-09-another-tidying-problem/",
    "title": "Another tidying problem",
    "description": "that ends up with a matched pairs test after tidying.",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2020-07-09",
    "categories": [],
    "contents": "\nIntroduction\nSome cars have a computer that records gas mileage since the last time the computer was reset. A driver is concerned that the computer on their car is not as accurate as it might be, so they keep an old-fashioned notebook and record the miles driven since the last fillup, and the amount of gas filled up, and use that to compute the miles per gallon. They also record what the car’s computer says the miles per gallon was.\nIs there a systematic difference between the computer’s values and the driver’s? If so, which way does it go?\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nThe data\nThe driver’s notebook has small pages, so the data look like this:\nFillup     1    2    3    4    5\nComputer 41.5 50.7 36.6 37.3 34.2\nDriver   36.5 44.2 37.2 35.6 30.5\nFillup     6    7    8    9   10\nComputer 45.0 48.0 43.2 47.7 42.2\nDriver   40.5 40.0 41.0 42.8 39.2\nFillup    11   12   13   14   15\nComputer 43.2 44.6 48.4 46.4 46.8\nDriver   38.8 44.5 45.4 45.3 45.7\nFillup    16   17   18   19   20\nComputer 39.2 37.3 43.5 44.3 43.3\nDriver   34.2 35.2 39.8 44.9 47.5\nThis is not very close to tidy. There are three variables: the fillup number (identification), the computer’s miles-per-gallon value, and the driver’s. These should be in columns, not rows. Also, there are really four sets of rows, because of the way the data was recorded. How are we going to make this tidy?\nMaking it tidy\nAs ever, we take this one step at a time, building a pipeline as we go: we see what each step produces before figuring out what to do next.\nThe first thing is to read the data in; these are aligned columns, so read_table is the thing. Also, there are no column headers, so we have to say that as well:\n\n\nmy_url <- \"https://raw.githubusercontent.com/nxskok/nxskok.github.io/master/gas-mileage.txt\"\ngas <- read_table(my_url, col_names = FALSE)\ngas\n\n\n# A tibble: 12 × 6\n   X1          X2    X3    X4    X5    X6\n   <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Fillup     1     2     3     4     5  \n 2 Computer  41.5  50.7  36.6  37.3  34.2\n 3 Driver    36.5  44.2  37.2  35.6  30.5\n 4 Fillup     6     7     8     9    10  \n 5 Computer  45    48    43.2  47.7  42.2\n 6 Driver    40.5  40    41    42.8  39.2\n 7 Fillup    11    12    13    14    15  \n 8 Computer  43.2  44.6  48.4  46.4  46.8\n 9 Driver    38.8  44.5  45.4  45.3  45.7\n10 Fillup    16    17    18    19    20  \n11 Computer  39.2  37.3  43.5  44.3  43.3\n12 Driver    34.2  35.2  39.8  44.9  47.5\n\nLonger first\nI usually find it easier to make the dataframe longer first, and then figure out what to do next. Here, that means putting all the data values in one column, and having a column of variable names indicating what each variable is a value of, thus:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\")\n\n\n# A tibble: 60 × 3\n   X1       var_name var_value\n   <chr>    <chr>        <dbl>\n 1 Fillup   X2             1  \n 2 Fillup   X3             2  \n 3 Fillup   X4             3  \n 4 Fillup   X5             4  \n 5 Fillup   X6             5  \n 6 Computer X2            41.5\n 7 Computer X3            50.7\n 8 Computer X4            36.6\n 9 Computer X5            37.3\n10 Computer X6            34.2\n# … with 50 more rows\n\nThe things in X1 are our column-names-to-be, and the values that go with them are in var_value. var_name has mostly served its purpose; these are the original columns in the data file, which we don’t need any more. So now, we make this wider, right?\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value)  \n\n\n# A tibble: 5 × 4\n  var_name Fillup    Computer  Driver   \n  <chr>    <list>    <list>    <list>   \n1 X2       <dbl [4]> <dbl [4]> <dbl [4]>\n2 X3       <dbl [4]> <dbl [4]> <dbl [4]>\n3 X4       <dbl [4]> <dbl [4]> <dbl [4]>\n4 X5       <dbl [4]> <dbl [4]> <dbl [4]>\n5 X6       <dbl [4]> <dbl [4]> <dbl [4]>\n\nOh. How did we get list-columns?\nThe answer is that pivot_wider needs to know which column each var_value is going to, but also which row. The way it decides about rows is to look at all combinations of things in the other columns, the ones not involved in the pivot_wider. The only one of those here is var_name, so each value goes in the column according to its value in X1, and the row according to its value in var_name. For example, the value 41.5 in row 6 of the longer dataframe goes into the column labelled Computer and the row labelled X2. But if you scroll down the longer dataframe, you’ll find there are four data values with the Computer-X2 combination, so pivot_wider collects them together into one cell of the output dataframe.\nThis is what the warning is about.\nspread handled this much less gracefully:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  spread(X1, var_value)  \n\n\nError: Each row of output must be identified by a unique combination of keys.\nKeys are shared for 60 rows:\n* 6, 21, 36, 51\n* 7, 22, 37, 52\n* 8, 23, 38, 53\n* 9, 24, 39, 54\n* 10, 25, 40, 55\n* 11, 26, 41, 56\n* 12, 27, 42, 57\n* 13, 28, 43, 58\n* 14, 29, 44, 59\n* 15, 30, 45, 60\n* 1, 16, 31, 46\n* 2, 17, 32, 47\n* 3, 18, 33, 48\n* 4, 19, 34, 49\n* 5, 20, 35, 50\n\nIt required a unique combination of values for the other variables in the dataframe, which in our case we have not got.\nAll right, back to this:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value)  \n\n\n# A tibble: 5 × 4\n  var_name Fillup    Computer  Driver   \n  <chr>    <list>    <list>    <list>   \n1 X2       <dbl [4]> <dbl [4]> <dbl [4]>\n2 X3       <dbl [4]> <dbl [4]> <dbl [4]>\n3 X4       <dbl [4]> <dbl [4]> <dbl [4]>\n4 X5       <dbl [4]> <dbl [4]> <dbl [4]>\n5 X6       <dbl [4]> <dbl [4]> <dbl [4]>\n\nThere is a mindless way to go on from here, and a thoughtful way.\nThe mindless way to handle unwanted list-columns is to throw an unnest at the problem and see what happens:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value)  %>% \n  unnest()\n\n\nWarning: Values are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = length` to identify where the duplicates arise\n* Use `values_fn = {summary_fun}` to summarise duplicates\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(Fillup, Computer, Driver)`\n# A tibble: 20 × 4\n   var_name Fillup Computer Driver\n   <chr>     <dbl>    <dbl>  <dbl>\n 1 X2            1     41.5   36.5\n 2 X2            6     45     40.5\n 3 X2           11     43.2   38.8\n 4 X2           16     39.2   34.2\n 5 X3            2     50.7   44.2\n 6 X3            7     48     40  \n 7 X3           12     44.6   44.5\n 8 X3           17     37.3   35.2\n 9 X4            3     36.6   37.2\n10 X4            8     43.2   41  \n11 X4           13     48.4   45.4\n12 X4           18     43.5   39.8\n13 X5            4     37.3   35.6\n14 X5            9     47.7   42.8\n15 X5           14     46.4   45.3\n16 X5           19     44.3   44.9\n17 X6            5     34.2   30.5\n18 X6           10     42.2   39.2\n19 X6           15     46.8   45.7\n20 X6           20     43.3   47.5\n\nThis has worked.1 The fillup numbers have come out in the wrong order, but that’s probably not a problem. It would also work if you had a different number of observations on each row of the original data file, as long as you had a fillup number, a computer value and a driver value for each one.\nThe thoughtful way to go is to organize it so that each row will have a unique combination of columns that are left. A way to do that is to note that the original data file has four “blocks” of five observations each:\n\n\ngas\n\n\n# A tibble: 12 × 6\n   X1          X2    X3    X4    X5    X6\n   <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Fillup     1     2     3     4     5  \n 2 Computer  41.5  50.7  36.6  37.3  34.2\n 3 Driver    36.5  44.2  37.2  35.6  30.5\n 4 Fillup     6     7     8     9    10  \n 5 Computer  45    48    43.2  47.7  42.2\n 6 Driver    40.5  40    41    42.8  39.2\n 7 Fillup    11    12    13    14    15  \n 8 Computer  43.2  44.6  48.4  46.4  46.8\n 9 Driver    38.8  44.5  45.4  45.3  45.7\n10 Fillup    16    17    18    19    20  \n11 Computer  39.2  37.3  43.5  44.3  43.3\n12 Driver    34.2  35.2  39.8  44.9  47.5\n\nEach set of three rows is one block. So if we number the blocks, each observation of Fillup, Computer, and Driver will have an X-something column that it comes from and a block, and this combination will be unique.\nYou could create the block column by hand easily enough, or note that each block starts with a row called Fillup and use this idea:\n\n\ngas %>% mutate(block = cumsum(X1==\"Fillup\"))\n\n\n# A tibble: 12 × 7\n   X1          X2    X3    X4    X5    X6 block\n   <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <int>\n 1 Fillup     1     2     3     4     5       1\n 2 Computer  41.5  50.7  36.6  37.3  34.2     1\n 3 Driver    36.5  44.2  37.2  35.6  30.5     1\n 4 Fillup     6     7     8     9    10       2\n 5 Computer  45    48    43.2  47.7  42.2     2\n 6 Driver    40.5  40    41    42.8  39.2     2\n 7 Fillup    11    12    13    14    15       3\n 8 Computer  43.2  44.6  48.4  46.4  46.8     3\n 9 Driver    38.8  44.5  45.4  45.3  45.7     3\n10 Fillup    16    17    18    19    20       4\n11 Computer  39.2  37.3  43.5  44.3  43.3     4\n12 Driver    34.2  35.2  39.8  44.9  47.5     4\n\nThis works because X1==\"Fillup\" is either true or false. cumsum takes cumulative sums; that is, the sum of all the values in the column down to and including the one you’re looking at. It requires numeric input, though, so it turns the logical values into 1 for TRUE and 0 for FALSE and adds those up. (This is the same thing that as.numeric does.) The idea is that the value of block gets bumped by one every time you hit a Fillup line.\nThen pivot-longer as before:\n\n\ngas %>% mutate(block = cumsum(X1==\"Fillup\")) %>% \n  pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\")\n\n\n# A tibble: 60 × 4\n   X1       block var_name var_value\n   <chr>    <int> <chr>        <dbl>\n 1 Fillup       1 X2             1  \n 2 Fillup       1 X3             2  \n 3 Fillup       1 X4             3  \n 4 Fillup       1 X5             4  \n 5 Fillup       1 X6             5  \n 6 Computer     1 X2            41.5\n 7 Computer     1 X3            50.7\n 8 Computer     1 X4            36.6\n 9 Computer     1 X5            37.3\n10 Computer     1 X6            34.2\n# … with 50 more rows\n\nand now you can check that the var_name - block combinations are unique for each value in X1, so pivoting wider should work smoothly now:\n\n\n(gas %>% mutate(block = cumsum(X1==\"Fillup\")) %>% \n  pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value) -> gas1)\n\n\n# A tibble: 20 × 5\n   block var_name Fillup Computer Driver\n   <int> <chr>     <dbl>    <dbl>  <dbl>\n 1     1 X2            1     41.5   36.5\n 2     1 X3            2     50.7   44.2\n 3     1 X4            3     36.6   37.2\n 4     1 X5            4     37.3   35.6\n 5     1 X6            5     34.2   30.5\n 6     2 X2            6     45     40.5\n 7     2 X3            7     48     40  \n 8     2 X4            8     43.2   41  \n 9     2 X5            9     47.7   42.8\n10     2 X6           10     42.2   39.2\n11     3 X2           11     43.2   38.8\n12     3 X3           12     44.6   44.5\n13     3 X4           13     48.4   45.4\n14     3 X5           14     46.4   45.3\n15     3 X6           15     46.8   45.7\n16     4 X2           16     39.2   34.2\n17     4 X3           17     37.3   35.2\n18     4 X4           18     43.5   39.8\n19     4 X5           19     44.3   44.9\n20     4 X6           20     43.3   47.5\n\nand so it does.\nSometimes a pivot_longer followed by a pivot_wider can be turned into a single pivot_longer with options (see the pivoting vignette for examples), but this appears not to be one of those.\nComparing the driver and the computer\nNow that we have tidy data, we can do an analysis. These are matched-pair data (one Computer and one Driver measurement), so a sensible graph would be of the differences, a histogram, say:\n\n\ngas1 %>% mutate(diff = Computer - Driver) %>% \n  ggplot(aes(x=diff)) + geom_histogram(bins=6)\n\n\n\n\nThere is only one observation where the driver’s measurement is much bigger than the computer’s; otherwise, there is not much to choose or the computer’s measurement is bigger. Is this something that would generalize to “all measurements”, presumably all measurements at fillup by this driver and this computer? The differences are not badly non-normal, so a \\(t\\)-test should be fine:\n\n\nwith(gas1, t.test(Computer, Driver, paired = TRUE))\n\n\n\n    Paired t-test\n\ndata:  Computer and Driver\nt = 4.358, df = 19, p-value = 0.0003386\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1.418847 4.041153\nsample estimates:\nmean of the differences \n                   2.73 \n\nIt is. The computer’s mean measurement is estimated to be between about 1.4 and 4.0 miles per gallon larger than the driver’s.\nReferences\nData from here, exercise 7.35.\nNaming of parts\nPivoting vignette from tidyr\n\nI did get away with using unnest the old-fashioned way, though. What I should have done is given below the second warning.↩︎\n",
    "preview": "posts/2020-07-09-another-tidying-problem/another-tidying-problem_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2021-11-07T20:06:26-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-14-understanding-the-result-of-a-chi-square-test/",
    "title": "Understanding the result of a chi-square test",
    "description": "Going beyond the chi-square statistic and its P-value",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2020-03-14",
    "categories": [],
    "contents": "\nIntroduction\nA chisquare test can be used for assessing whether there is association between two categorical variables. The problem it has is that knowing that an association exists is only part of the story; we want to know what is making the association happen. This is the same kind of thing that happens with analysis of variance: a significant \\(F\\)-test indicates that the group means are not all the same, but not which ones are different.\nRecently I discovered that R’s chisq.test has something that will help in understanding this.\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nwhich I always seem to need for something.\nExample\nHow do males and females differ in their choice of eyewear (glasses, contacts, neither), if at all? Some data (frequencies):\n\n\neyewear <- tribble(\n  ~gender, ~contacts, ~glasses, ~none,\n  \"female\", 121, 32, 129,\n  \"male\", 42, 37, 85\n)\neyewear\n\n\n# A tibble: 2 × 4\n  gender contacts glasses  none\n  <chr>     <dbl>   <dbl> <dbl>\n1 female      121      32   129\n2 male         42      37    85\n\nIt is a little difficult to compare since there are fewer males than females here, but we might suspect that males proportionately are more likely to wear glasses and less likely to wear contacts than females.\nDoes the data support an association at all?\n\n\neyewear %>% select(-gender) %>% chisq.test() -> z\nz\n\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 17.718, df = 2, p-value = 0.0001421\n\nThere is indeed an association.\nCoding note: normally chisq.test accepts as input a matrix (eg. output from table), but it also accepts a data frame as long as all the columns are frequencies. So I had to remove the gender column first.1\nSo, what kind of association? chisq.test has, as part of its output, residuals. Maybe you remember calculating these tests by hand, and have, lurking in the back of your mind somewhere, “observed minus expected, squared, divide by expected”. There is one of these for each cell, and you add them up to get the test statistic. The “Pearson residuals” in a chi-squared table are the signed square roots of these, where the sign is negative if observed is less than expected:\n\n\neyewear\n\n\n# A tibble: 2 × 4\n  gender contacts glasses  none\n  <chr>     <dbl>   <dbl> <dbl>\n1 female      121      32   129\n2 male         42      37    85\n\nz$residuals\n\n\n      contacts   glasses       none\n[1,]  1.766868 -1.760419 -0.5424069\n[2,] -2.316898  2.308440  0.7112591\n\nThe largest (in size) residuals make the biggest contribution to the chi-squared test statistic, so these are the ones where observed and expected are farthest apart. Hence, here, fewer males wear contacts and more males wear glasses compared to what you would expect if there were no association between gender and eyewear.\nI am not quite being sexist here: the male and female frequencies are equally far away from the expected in absolute terms:\n\n\neyewear\n\n\n# A tibble: 2 × 4\n  gender contacts glasses  none\n  <chr>     <dbl>   <dbl> <dbl>\n1 female      121      32   129\n2 male         42      37    85\n\nz$expected\n\n\n      contacts glasses      none\n[1,] 103.06278 43.6278 135.30942\n[2,]  59.93722 25.3722  78.69058\n\nbut the contribution to the test statistic is more for the males because there are fewer of them altogether.\n\nThis behaviour undoubtedly comes from the days when matrices had row names which didn’t count as a column.↩︎\n",
    "preview": {},
    "last_modified": "2021-11-07T21:43:05-05:00",
    "input_file": {}
  }
]
