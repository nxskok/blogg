[
  {
    "path": "posts/2021-12-04-kommentar/",
    "title": "Kommentar",
    "description": "Or, Comments, in other words.",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2021-12-04",
    "categories": [],
    "contents": "\nI used to have Disqus comments on my blog (the old one), but there are good reasons not to do that, among them the question of who owns the comments on your blog posts, yes, really. I looked vaguely into utteranc.es, but I could never get it to work for me, so I left the comments idea aside.\nThen, today, I was browsing Twitter while waiting for something else, I got directed to a blog post by Athanasia Mowinckel. Her blog is Hugo, which mine is not, but there was a link to a Twitter post by Shannon Pileggi linking to this blog post by Joel Nitta, and following the procedure described there worked smoothly either side of dinner, and would have worked more smoothly had I actually read what he wrote…\nAnyway, feel free to comment below, as I can now say.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-04T20:48:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-14-tidy-simulation/",
    "title": "Tidy simulation",
    "description": "Using rowwise to save calculation, estimate power or test size, bootstrap distributions",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2021-11-26",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nIntroduction\nTo see what might happen when a process is repeated many times, we can calculate. Or we can physically re-run the process many times, and count up the results: simulation of the process.\nThis can be applied to estimating probabilities, obtaining bootstrap distributions (for example when assessing normality), or estimating the power or size of tests.\nI want my simulations here to be reproducible, so I will set the random number seed first:\n\n\nset.seed(457299)\n\n\n\nTossing a coin\nImagine we toss a fair coin 10 times. How likely are we to get 8 or more heads? If you remember the binomial distribution, you can work it out. But if you don’t? Make a virtual coin, toss it 10 times, count the number of heads, repeat many times, see how many of those are 8 or greater.\nLet’s set up our virtual coin first:\n\n\ncoin <- c(\"H\", \"T\")\n\n\n\nand, since getting a head on one toss doesn’t prevent a head on others, ten coin tosses would be a sample of size 10 with replacement from this coin:\n\n\nsample(coin, 10, replace = TRUE)\n\n\n [1] \"H\" \"T\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"T\" \"T\"\n\nSeven heads this time.\nI have a mechanism I use for “tidy simulation”:\nset up a dataframe with a column called sim to label the simulations\nwork rowwise\nfor each sim, do one copy of the thing you’ll be doing many times (in this case, simulating 10 coin tosses)\ncalculate whatever you want to calculate for each sim\nsummarize the results\nFor this problem, the code looks like this:\n\n\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(coin, 10, replace = TRUE))) %>% \n  mutate(heads = sum(my_sample == \"H\")) %>% \n  count(heads >= 8)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `heads >= 8`     n\n  <lgl>        <int>\n1 FALSE          946\n2 TRUE            54\n\nIt is probably a good idea to run this one line at a time (to see what it does, and later as you develop your own).\nIn this case, 54 of the 1000 simulated sets of 10 coin tosses gave at least 8 heads, so our estimate of the probability of getting 8 or more heads in 10 tosses of a fair coin is 0.054.\nSome notes about the code:\nI am using 1000 simulations as my “many” repeats of tossing a coin 10 times. A larger number would give a more accurate answer, but would take longer to run.1\nworking rowwise allows us to treat each row of the dataframe we are building as an independent entity. This makes the coding in the two mutates that follow much easier to follow, because our mental model only has to work one row at a time.2\nmy_sample behaves like one sample of 10 coin tosses, though in fact it is a whole column of samples of 10 coin tosses. It is a vector of length 10, so to get it into one cell of our dataframe, we wrap it in list, making the whole column a list-column.\nOnce again thinking of my_sample as a single sample, we then count the number of heads in it. I could use count, or table, but I don’t want to get caught by samples with no heads or no tails. This way counts 1 for each H in the sample, then adds up the counts.3\nFinally, count up the number of simulated sets of 10 coin tosses that had 8 or more heads. count accepts a logical condition as well as a column. (Behind the scenes it constructs a column of TRUE and FALSE first, and then counts that.)\nIn this case, we know the right answer:4\n\n\npbinom(7, 10, 0.5, lower.tail = FALSE)\n\n\n[1] 0.0546875\n\nOur simulation came out very close to this.\nAside:5 we can work out how accurate our simulation might be by noting that our 1000 simulations are also like Bernoulli trials: each one gives us 8 or more heads or it doesn’t, with unknown probability that is precisely the thing that we are trying to estimate. Thus:\n\n\nbinom.test(54, 1000)\n\n\n\n    Exact binomial test\n\ndata:  54 and 1000\nnumber of successes = 54, number of trials = 1000, p-value <\n2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.04082335 0.06987401\nsample estimates:\nprobability of success \n                 0.054 \n\ntells us, with 95% confidence, that the probability of 8 or more heads is between 0.041 and 0.070. To nail it down more precisely, use more than 1000 simulations.\nHow long is my longest suit?\nIn the game of bridge, each player, in two partnerships of 2, receives a hand of 13 cards randomly dealt from the usual deck of 52 cards. There is then an “auction” in which the two partnerships compete for the right to name the trump suit and play the hand. The bids in this auction are an undertaking to win a certain number of the 13 tricks with the named suit as trumps.6 Your partner cannot see your cards, and so in the bidding you have to share information about the strength and suit distribution of your hand using standard methods7 (you are not allowed to deceive your opponents), so that as a partnership you can decide how many tricks you can win between you.\nOne of the considerations in the bidding is the length of your longest suit, that is, the suit you hold the most cards in. The longest suit might have only 4 cards (eg. if you have 4 spades and 3 of each of the other suits), but if you are lucky8 you might be dealt a hand with 13 cards all of the same suit and have a longest suit of 13 cards. Evidently something in between those is more likely, but how likely?\nFor a simulation, we need to set up a deck of cards and select 13 cards from it without replacement (since you can’t draw the same card twice in the same hand). The only thing that matters here is the suits, so we’ll set up a deck with only suits and no denominations like Ace or King. (This will make the sampling without replacement look a bit odd.)\n\n\ndeck <- c(rep(\"S\", 13), rep(\"H\", 13),\n          rep(\"D\", 13), rep(\"C\", 13))\ndeck\n\n\n [1] \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" \"H\" \"H\" \"H\"\n[17] \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"H\" \"D\" \"D\" \"D\" \"D\" \"D\" \"D\"\n[33] \"D\" \"D\" \"D\" \"D\" \"D\" \"D\" \"D\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\"\n[49] \"C\" \"C\" \"C\" \"C\"\n\nand deal ourselves a hand of 13 cards thus:\n\n\nhand <- sample(deck, 13, replace = FALSE)\nhand\n\n\n [1] \"D\" \"C\" \"C\" \"D\" \"S\" \"H\" \"S\" \"S\" \"D\" \"S\" \"H\" \"H\" \"H\"\n\n(note, for example, that the four Hearts in this hand are actually four different ones of the thirteen H in deck, since we are sampling without replacement. I could have labelled them by which Heart they were, but that would have made counting them more difficult.)\nThen count the number of cards in each suit:\n\n\ntab <- table(hand)\ntab\n\n\nhand\nC D H S \n2 3 4 4 \n\nThis time the longest suit has four cards:\n\n\nmax(tab)\n\n\n[1] 4\n\nUsing table is safe here, because we don’t care whether there are any suits with no cards in the hand, only about the greatest number of cards in any suit that we have cards in.9\nAll of that leads us to this:\n\n\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(hand = list(sample(deck, 13, replace = FALSE))) %>% \n  mutate(suits = list(table(hand))) %>% \n  mutate(longest = max(suits)) %>% \n  count(longest)\n\n\n# A tibble: 5 × 2\n# Rowwise: \n  longest     n\n    <int> <int>\n1       4   354\n2       5   435\n3       6   171\n4       7    38\n5       8     2\n\nNote: the hands, and the tables of how many cards a hand has in each suit, are more than single numbers, so they need to be wrapped in list.\nThe most likely longest suit has 5 cards in it, a bit less than half the time. According to this, a longest suit of 8 cards happens about once in 500 hands, and longer longest suits are even less likely. (To estimate these small probabilities accurately, you need a lot of simulations, like, way more than 1000.)\nAside: the standard way of assessing hand strength is via high-card points: 4 for an ace, 3 for a king, 2 for a queen and one for a jack. All the other cards count zero. To simulate the number of points you might get in a hand, build a deck with the points for each card. There are four cards of each rank, and nine ranks that are worth no points:\n\n\ndeck <- c(rep(4,4), rep(3,4),\n          rep(2,4), rep(1,4), rep(0, 36))\n\n\n\nThe simulation process after that is a lot like before:\n\n\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(hand = list(sample(deck, 13, replace = FALSE))) %>% \n  mutate(points = sum(hand)) -> d\nd\n\n\n# A tibble: 1,000 × 3\n# Rowwise: \n     sim hand       points\n   <int> <list>      <dbl>\n 1     1 <dbl [13]>     13\n 2     2 <dbl [13]>     10\n 3     3 <dbl [13]>     11\n 4     4 <dbl [13]>      4\n 5     5 <dbl [13]>      4\n 6     6 <dbl [13]>      6\n 7     7 <dbl [13]>     13\n 8     8 <dbl [13]>      1\n 9     9 <dbl [13]>     13\n10    10 <dbl [13]>      7\n# … with 990 more rows\n\nI stopped it there, partly to show what the dataframe looks like at this point (a hand of 13 point values, and a points total that is the sum of these) and partly because I wanted to do about three things with this, and it made sense to save what we have done thus far.\nFirst, a bar chart of how likely each number of points is:\n\n\nggplot(d, aes(x = points)) + geom_bar()\n\n\n\n\nIf you do more simulations, you can check whether the shape is indeed smooth (I’m guessing it is). The average number of points is 10 (there are 40 points in the deck and yours is one of four hands) and the distribution is right-skewed because it is possible, though rather unlikely, to get over 20 points.\nIn most bidding systems, having 13 points justifies opening the bidding (making the first bid in the auction if everyone has passed on their turn before you). How likely is that?\n\n\nd %>% count(points >= 13)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `points >= 13`     n\n  <lgl>          <int>\n1 FALSE            749\n2 TRUE             251\n\nOnly about a quarter of the time.\nHaving 20 or more points qualifies your hand for an opening bid at the 2-level.10 How likely is that?\n\n\nd %>% count(points >= 20)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `points >= 20`     n\n  <lgl>          <int>\n1 FALSE            985\n2 TRUE              15\n\nA bit of a rarity, less than a 2% shot.\nBootstrapping a sampling distribution\nTo return to the messy world of actual applied statistics: there are a lot of procedures based on an assumption of the right things having normal distributions.11 One of the commonest questions is whether we should be using the normal-theory procedure or something else (non-parametric, maybe). Let’s take an example. The data here are information about Toronto Blue Jays baseball games from the early part of the 2015 season:\n\n\nmy_url <- \"http://ritsokiguess.site/datafiles/jays15-home.csv\"\njays <- read_csv(my_url)\njays\n\n\n# A tibble: 25 × 21\n     row  game date      box    team  venue opp   result  runs Oppruns\n   <dbl> <dbl> <chr>     <chr>  <chr> <lgl> <chr> <chr>  <dbl>   <dbl>\n 1    82     7 Monday, … boxsc… TOR   NA    TBR   L          1       2\n 2    83     8 Tuesday,… boxsc… TOR   NA    TBR   L          2       3\n 3    84     9 Wednesda… boxsc… TOR   NA    TBR   W         12       7\n 4    85    10 Thursday… boxsc… TOR   NA    TBR   L          2       4\n 5    86    11 Friday, … boxsc… TOR   NA    ATL   L          7       8\n 6    87    12 Saturday… boxsc… TOR   NA    ATL   W-wo       6       5\n 7    88    13 Sunday, … boxsc… TOR   NA    ATL   L          2       5\n 8    89    14 Tuesday,… boxsc… TOR   NA    BAL   W         13       6\n 9    90    15 Wednesda… boxsc… TOR   NA    BAL   W          4       2\n10    91    16 Thursday… boxsc… TOR   NA    BAL   W          7       6\n# … with 15 more rows, and 11 more variables: innings <dbl>,\n#   wl <chr>, position <dbl>, gb <chr>, winner <chr>, loser <chr>,\n#   save <chr>, game time <time>, Daynight <chr>, attendance <dbl>,\n#   streak <chr>\n\nThere is a lot of information here, but we’re going to focus on the attendances over near the right side, and in particular, we’re interested in the mean attendance over all games of which these are a sample (“early-season Blue Jays games in the years between 2010 and 2019”, or something like that). There are, of course, lots of reasons that attendances might vary (opposition, weather, weekend vs. weekday, etc.) that we are going to completely ignore here.\nThe normal12 way to estimate a population mean is to use the confidence interval based on the one-sample \\(t\\)-test, but before we jump into that, we should look at a graph of the attendances:\n\n\nggplot(jays, aes(x = attendance)) + geom_histogram(bins = 6)\n\n\n\n\nWell, that doesn’t look much like a normal distribution. It’s very much skewed to the right. There seem to be two13 schools of thought as to what we should do now:\nwe have a large enough sample (\\(n = 25\\)) so that we should get enough help from the central limit theorem (also expressed as “the \\(t\\)-test is robust to non-normality”) and therefore the \\(t\\)-procedure should be fine.\nthis distribution is a long way from being normal, so there is no way we should use a \\(t\\)-procedure, instead using a sign test or signed-rank test,14 inverted to get a confidence interval for the median attendance.\nBoth of these have an air of handwavery about them. How do we decide between them? Well, let’s think about this a little more carefully. When it comes to getting confidence limits, it all depends on the sampling distribution of the sample mean. If that is close enough to normal, the \\(t\\)-interval is good. But this comes from repeated sampling. You conceptualize it by imagining taking lots of samples from the same population, working out the mean of each sample, and making something like a histogram or normal quantile plot of those. But but — we only have the one sample we have. How to think about possible sample means we might get?\nA way around this is to use the bootstrap. The idea is to think of the sample we have as a population (resembling, we hope, the population we want to make inferences about of “all possible attendances”) and to take samples from our sample(!) of the same size as the sample we had. If we do this the obvious way (without replacement), we’ll get back the original sample we had, every time. So what we do instead is to sample from our sample, but with replacement so as to get a different set of values each time, with some values missing and some values repeated. Like this:\n\n\ns <- sample(jays$attendance, replace = TRUE)\nsort(s)\n\n\n [1] 15062 15062 15086 15086 15086 15168 16402 17264 17276 17276 18581\n[12] 19217 21519 21519 21519 21519 21519 29306 29306 33086 34743 37929\n[23] 42419 42917 44794\n\nSorting the sample reveals that the first two values and the next three are repeats, so there must be some values from the original sample that are missing. (This is the only reason I sorted them.)\nThe original data had a mean of\n\n\njays %>% summarise(mean_att = mean(attendance))\n\n\n# A tibble: 1 × 1\n  mean_att\n     <dbl>\n1   25070.\n\nbut the bootstrap sample has a mean of\n\n\nmean(s)\n\n\n[1] 23946.44\n\ndifferent; if we were to take more bootstrap samples, and find the mean of each one, we would get a sense of the sampling distribution of the sample mean. That is to say, we simulate the bootstrapped sampling distribution of the sample mean. Given what we’ve seen in the other simulations, the structure of the code below ought to come as no surprise:\n\n\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(s = list(sample(jays$attendance, replace = TRUE))) %>% \n  mutate(m = mean(s)) -> d\nd\n\n\n# A tibble: 1,000 × 3\n# Rowwise: \n     sim s               m\n   <int> <list>      <dbl>\n 1     1 <dbl [25]> 22328.\n 2     2 <dbl [25]> 29364.\n 3     3 <dbl [25]> 24194.\n 4     4 <dbl [25]> 22907.\n 5     5 <dbl [25]> 22604.\n 6     6 <dbl [25]> 22905.\n 7     7 <dbl [25]> 23957.\n 8     8 <dbl [25]> 23567.\n 9     9 <dbl [25]> 24944.\n10    10 <dbl [25]> 27150.\n# … with 990 more rows\n\nIn words, set up the 1000 simulations and work rowwise as before, then take (for each row) a bootstrap sample of the attendances, and then take the mean of it. I’ve saved the resulting dataframe so that we can look at it and then do something else with it. The column s containing the samples is a list-column again.\nOur question was whether this bootstrapped sampling distribution of the sample mean looked like a normal distribution. To see that, a normal quantile plot is the thing:\n\n\nggplot(d, aes(sample = m)) + stat_qq() + stat_qq_line()\n\n\n\n\nThat is very close to a normal distribution, and so in fact the \\(t\\)-procedure really is fine and the first school of thought is correct (and now we have evidence, no hand-waving required):\n\n\nt.test(jays$attendance)\n\n\n\n    One Sample t-test\n\ndata:  jays$attendance\nt = 11.389, df = 24, p-value = 3.661e-11\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 20526.82 29613.50\nsample estimates:\nmean of x \n 25070.16 \n\nA 95% confidence interval for the mean attendance goes from 20500 to 29600.\nAnother way to go is to use the bootstrapped sampling distribution directly, entirely bypassing all the normal theory, and just take the middle 95% of it:\n\n\nd %>% ungroup() %>% summarize(ci = quantile(m, c(0.025, 0.975)))\n\n\n# A tibble: 2 × 1\n      ci\n   <dbl>\n1 20978.\n2 29668.\n\n21000 to 29700, not that different (given the large amount of variability) from the \\(t\\)-interval. There are better ways to get the interval rather than using sample quantiles; see for example here. But this will do for now.\nThe ungroup in the code is there because the dataframe d is still rowwise: everything we do with d will still be done one row at a time. But now we want to work on the whole column m, so we have to undo the rowwise first. rowwise is a special case of group_by (a sort of group-by-rows), so you undo rowwise in the same way that you undo group_by.\nPower by simulation\nPower of a test\nR has things like power.t.test that will allow you to calculate the power of one- and two-sample \\(t\\)-tests for normally-distributed populations. But what if you want to find out the power of some other test, or of a \\(t\\)-test under other assumptions about the population distribution? We need to have a mechanism for simulating power.\nLet’s start off simple with one where we can check the answer. Let’s suppose that our population is normal with mean 110 and SD 30, and we have a sample of size 20. How likely are we to (correctly) reject the null hypothesis than the mean is 100, in favour of the alternative that the mean is greater than 100?\nThe exact answer is this, using \\(\\alpha = 0.05\\):\n\n\npower.t.test(n = 20, delta = 110 - 100, sd = 30, type = \"one.sample\", alternative = \"one.sided\")\n\n\n\n     One-sample t test power calculation \n\n              n = 20\n          delta = 10\n             sd = 30\n      sig.level = 0.05\n          power = 0.4178514\n    alternative = one.sided\n\nSimulating this gives a rather more detailed handle on what is actually going on. The idea is to draw lots of samples from the truth, test the (incorrect) null, and grab the P-value each time, then count how many of those P-values are less than 0.05 (or whatever your \\(\\alpha\\) is). The true population here is normal with mean 110 and SD 30, and our sample size is 20:\n\n\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(20, 110, 30))) %>% \n  mutate(t_test = list(t.test(sample, mu = 100, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              5676\n2 TRUE               4324\n\nThe estimated power is 42.6%, a little bigger than but not far from the correct answer.15\nI did this in several steps. After the rowwise, I drew a (list-column of) samples from the true population, then I ran a one-sample \\(t\\)-test to test whether the population mean is greater than 100 (and saved all the t.test output), then I extracted the P-value, then I counted how many of those P-values were 0.05 or less. I laid it out this way so that you can adapt for your purposes; you could change the population distribution, or the test, and the procedure will still work.16\nThe usual practical reason for wanting to get power is before an experiment is run: this is the sample size you plan to use, this is what you think the (true) population is, this is the null hypothesis you would like to reject. Except that this is not quite what usually happens in practice; usually you have a target power in mind, like 0.80, and you want to know what sample size you need in order to achieve that power.\nWith power.t.test, this is as simple as putting in power and leaving out n:\n\n\npower.t.test(power = 0.80, delta = 110 - 100, sd = 30, type = \"one.sample\", alternative = \"one.sided\")\n\n\n\n     One-sample t test power calculation \n\n              n = 57.02048\n          delta = 10\n             sd = 30\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nand the sample size has to be 58 (rounding up).\nBut by simulation, n has to be input to the simulation and power is the output. So the best we can do is to try different sample sizes and see which one gets us closest to the power we are aiming for. A sample size of 20 is, we know, too small, but what about 40? One change to the previous code:\n\n\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(40, 110, 30))) %>% \n  mutate(t_test = list(t.test(sample, mu = 100, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              3388\n2 TRUE               6612\n\nThis estimates the power to be 66%, not big enough, so the sample size needs to be bigger still.\nAnother problem is that this is only an estimate of the power, based on “only” 10,000 simulations. It could be that the power for a sample size of 40 is higher than this. But how much higher? The binom.test idea from earlier gives us a confidence interval for the true power:\n\n\nbinom.test(6612, 10000)\n\n\n\n    Exact binomial test\n\ndata:  6612 and 10000\nnumber of successes = 6612, number of trials = 10000, p-value\n< 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.6518275 0.6704785\nsample estimates:\nprobability of success \n                0.6612 \n\nThe power (between 0.652 and 0.670) is evidently not high enough yet, so we need a bigger sample size. 60?\n\n\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(60, 110, 30))) %>% \n  mutate(t_test = list(t.test(sample, mu = 100, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              1848\n2 TRUE               8152\n\nThat looks pretty close. What does the confidence interval look like?\n\n\nbinom.test(8152, 10000)\n\n\n\n    Exact binomial test\n\ndata:  8152 and 10000\nnumber of successes = 8152, number of trials = 10000, p-value\n< 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8074513 0.8227647\nsample estimates:\nprobability of success \n                0.8152 \n\nThe 95% CI for the true power goes from 0.807 to 0.823, which is a little too high, so the sample size I need is a little under 60. Now you see the reason for doing 10,000 simulations instead of only 1000: I’ve nailed down the true power rather accurately. Compare this:\n\n\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(60, 110, 30))) %>% \n  mutate(t_test = list(t.test(sample, mu = 100, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE               189\n2 TRUE                811\n\nbinom.test(811, 1000)\n\n\n\n    Exact binomial test\n\ndata:  811 and 1000\nnumber of successes = 811, number of trials = 1000, p-value <\n2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.7853323 0.8348215\nsample estimates:\nprobability of success \n                 0.811 \n\nWith this many simulations, we see that a sample size of 60 is as close as we are going to get, since 0.80 is inside this confidence interval.\nSize of a test\nIf the true mean and the hypothesized mean are the same, then the null hypothesis is actually true and the probability of (now incorrectly) rejecting it should be 0.05. In a situation where the test is properly calibrated, this is not very interesting:\n\n\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(20, 100, 30))) %>% \n  mutate(t_test = list(t.test(sample, mu = 100, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              9487\n2 TRUE                513\n\nbinom.test(513, 10000)\n\n\n\n    Exact binomial test\n\ndata:  513 and 10000\nnumber of successes = 513, number of trials = 10000, p-value <\n2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.04705735 0.05580631\nsample estimates:\nprobability of success \n                0.0513 \n\nNow the true mean and the null mean are both 100, and the population distribution is normal, so the \\(t\\)-test must be appropriate, and the “power”, that is to say, the probability of a type I error, could indeed be 0.05.\nBut what if the population distribution is not normal? Then we have the Central Limit Theorem, which says that everything should still behave well “for large samples”, without actually telling us how big the sample has to be. One way to assess whether our sample is big enough, if we have data, is to estimate the bootstrap sampling distribution of the sample mean. If we don’t have data, we can suggest a distributional form for the population distribution, and see how the test behaves in that case: does the test still reject 5% of the time, when the null is true?\nTo be specific, let’s suppose we’re taking a sample of size 20 from the very right-skewed exponential distribution with mean 100. Does the \\(t\\)-test for the mean reject 5% of the time when the null mean is 100?17\n\n\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(sample = list(rexp(20, 1/100))) %>% \n  mutate(t_test = list(t.test(sample, mu = 100, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n\n\n# A tibble: 2 × 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              9802\n2 TRUE                198\n\nbinom.test(198, 10000)\n\n\n\n    Exact binomial test\n\ndata:  198 and 10000\nnumber of successes = 198, number of trials = 10000, p-value <\n2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.01716007 0.02272454\nsample estimates:\nprobability of success \n                0.0198 \n\nClearly it rejects too little of the time (confidence interval for type I error probability from 0.017 to 0.023). So a sample of size 20 is not big enough for the Central Limit Theorem to work in this case.\nAnother use for simulation is to understand the sampling distribution of a test statistic when we do not have theory to guide us. Let’s return to our normal population with mean 100 and SD 30. Suppose we now want to reject a mean of 100 in favour of the mean being greater than 100 if the sample maximum is large enough. How large should the sample maximum be?\nThe procedure is to generate samples from the truth (in this case the null is true), find the maximum of each simulated sample, and then find the 95th percentile of the simulated maxima:\n\n\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(20, 100, 30))) %>% \n  mutate(sample_max = max(sample)) %>% \n  ungroup() %>% \n  summarize(pp = quantile(sample_max, 0.95))\n\n\n# A tibble: 1 × 1\n     pp\n  <dbl>\n1  184.\n\nWe should reject a mean of 100 if the sample maximum is 184 or greater.\nAnd now we can estimate the power of this test if the mean is actually 110:\n\n\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(20, 110, 30))) %>% \n  mutate(sample_max = max(sample)) %>% \n  ungroup() %>% \n  count(sample_max >= 184)\n\n\n# A tibble: 2 × 2\n  `sample_max >= 184`     n\n  <lgl>               <int>\n1 FALSE                 871\n2 TRUE                  129\n\nThe power is now only about 13%, much less than for the test based on the mean (which was about 42%). The reason for the ungroup was that I wanted to count something for the whole dataframe, not one row at a time, so I had to undo the rowwise.\nFinal remarks\nThere is a lot of repetitiousness here. It would almost certainly be better to abstract the ideas of the simulation away into a function (that might have inputs the true parameter(s), the null parameter, the test and the population distribution), but one of the things I wanted to get across was that these all work the same way with a few small changes, which doesn’t come across quite so clearly by changing inputs to a function.\n\nI will need to knit the multiple simulations in this blog post before I put it up, so I am sticking mostly with 1000, but you may be more patient than I am.↩︎\nYou can also do this with map from purrr, but I find the code more difficult to follow.↩︎\nMy base R heritage sometimes shows through.↩︎\nThe 7 is because using lower.tail = FALSE gives a probability strictly greater than the first input.↩︎\nBut one that is used again later.↩︎\nAdd six to the number in the bid to determine how many tricks the bidder and their partner are promising to win. Thus if you bid “two clubs” you are undertaking to win 8 of the 13 tricks between you with clubs as trumps.↩︎\nIn order to convey information, most bids say something about hand strength and the length of the suit bid, according to a system like Standard American or ACOL (British).↩︎\nThat is to say, out-of-this-universe lucky.↩︎\nWe could use a similar approach to estimate the probability of being dealt a void, a suit with no cards in it, but we would have to be more careful counting. Counting the number of different suits represented in the hand and seeing whether it is less than 4 would be one way.↩︎\nIn Standard American, 2 notrumps if you have no long or short suits, 2 clubs if you do, or your hand is stronger than 21 points. In one bidding system I know of, the lowest bid of 1 club is reserved for really strong hands like this!↩︎\nBecause that was easier to develop theory for.↩︎\nJoke. You may laugh.↩︎\nA third school would say “do a Bayesian analysis with suitable prior and likelihood model”, but that’s for another discussion.↩︎\nThere are problems with this, too, that I will go into another time.↩︎\nI’m doing 10,000 simulations this time.↩︎\nThe part of the test output with the P-value in it might not be called p.value in your case. Investigate.↩︎\nrexp’s second input is the “rate”, the reciprocal of the mean.↩︎\n",
    "preview": "posts/2021-11-14-tidy-simulation/tidy-simulation_files/figure-html5/unnamed-chunk-15-1.png",
    "last_modified": "2021-11-26T19:26:52-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-07-welcome/",
    "title": "Welcome",
    "description": "The new home of my blog",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2021-11-07",
    "categories": [],
    "contents": "\nWelcome to the new home for my blog. I have decided to move from blogdown to distill, since it seems to be easier to maintain this way. There will eventually be links to at least some of the old posts, and, I hope, some new ones.\nThe old blog is still up (and will remain so) here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-07T17:56:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-density-plots/",
    "title": "Density plots",
    "description": "An alternative to histograms and boxplots",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2021-10-16",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nSome data\nWe will need some data to illustrate the plots. I will use some data on physical and physiological measurements on 202 Australian elite athletes:\n\n\nmy_url <- \"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes <- read_tsv(my_url)\nathletes\n\n\n# A tibble: 202 × 13\n   Sex   Sport   RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM\n   <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl>\n 1 fema… Netb…  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1\n 2 fema… Netb…  4.15   6    38    12.7    59  21.2 110.     25.3  47.1\n 3 fema… Netb…  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4\n 4 fema… Netb…  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8\n 5 fema… Netb…  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0\n 6 fema… Netb…  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4\n 7 fema… Netb…  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1\n 8 fema… Netb…  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4\n 9 fema… Netb…  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0\n10 fema… Netb…  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6\n# … with 192 more rows, and 2 more variables: Ht <dbl>, Wt <dbl>\n\nThe histogram and the boxplot\nThe histogram goes back to Karl Pearson. It offers a simple way to visualize a single quantitative, continuous distribution. For example, a histogram of the weights of all the athletes:\n\n\nggplot(athletes, aes(x = Wt)) + geom_histogram(bins = 8)\n\n\n\n\nA histogram works by dividing the range of the quantitative variable into discrete intervals or “bins”, often of the same width. On the above histogram, the tallest bar goes with a bin from about 65 to about 80 (kg). The height of each histogram bar is the number of observations within that bin.\nEvidently, the choice of how many bins to use may have an impact on how the histogram looks. If you use more bins:\n\n\nggplot(athletes, aes(x = Wt)) + geom_histogram(bins = 20)\n\n\n\n\nyou get a more detailed picture of the distribution, at the expense of getting a less smooth picture. In this picture, the distribution appears to have something like three peaks, but you would probably say that these only appeared because we chose so many bins. On the other hand, if you have fewer bins:\n\n\nggplot(athletes, aes(x = Wt)) + geom_histogram(bins = 3)\n\n\n\n\nyou get a smooth picture all right, but you lose almost all of the detail.\nHadley Wickham, in “R For Data Science”, says “You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns.” This may be the reason that there is no default number of bins on geom_histogram (well, there is, but the default is way too many). The right number of bins for you and your histogram depends on the story you think the data are trying to tell.\nA boxplot is another way to make a visual of a quantitative variable. The boxplot was popularized by John Tukey, but is really a descendant of the “range bar” of Mary Eleanor Spear. Here’s how it looks for the weights:\n\n\nggplot(athletes, aes(x = 1, y = Wt)) + geom_boxplot()\n\n\n\n\nA boxplot is characterized by a box, two whiskers and some observations plotted individually. The line across the middle of the box is at the median; the bottom and top of the box are the first and third quartiles, so that the height of the box is the interquartile range; the whiskers join the box to the outermost “non-extreme” observations, and any “extreme” observations are plotted individually. (The usual criterion for “extreme” is more than 1.5 times the inter-quartile range beyond the quartiles.)\nWhere a boxplot really shines is in comparing several groups, for example, comparing the weights of athletes who play different sports:\n\n\nggplot(athletes, aes(x = Sport, y = Wt)) + geom_boxplot()\n\n\n\n\nThe gymnasts (Gym) are the least heavy on average, and also have a very small spread. (In this dataset, the gymnasts are all female.) The field athletes (Field) and the water polo players (WPolo) are the heaviest on average. There are some outliers (the heavy track sprinter (TSprnt) and the light netball player), and the rowers’ (Row) distribution has a long lower tail (the low extreme point may be an outlier or part of the long tail).\nThe value of the histogram and the boxplot is that they are easy to draw by hand. For example, you could first draw a stem-and-leaf plot of the data, and use the information there to count the number of observations in bins, or to work out the quartiles and find the extreme observations. (Many of Tukey’s other innovations were designed to be quick and simple, readily usable on a 1960s factory floor.) This is good if you were learning or using statistical methods in the days before computers, or you are pretending you are (not mentioning stats classes in Psychology at all, oh dear me no). But with access to R, there is no need to restrict ourselves to graphs that are easy to draw.\nDensity estimation\nThe histogram is a rather crude example of a “density estimator”: the number of observations per unit interval of (in our case) kilogram of weight. A basic way of estimating density at a certain weight, say 80 kg, is to take the number of observations in the histogram bin that includes 80, and then to divide it by the width of the bin.\nA limitation of the above method is that the estimate is the same all the way across the bin, giving a discontinuous estimate of density when the underlying density curve ought (you would expect) to be smooth.\nSomething that will give a smooth answer is kernel density estimation. For any given (athlete’s) weight \\(x\\), we choose a distribution centred at \\(x\\) (say normal) and choose a standard deviation for that distribution (there are rules of thumb for doing this). This is called a kernel. To estimate the density at \\(x\\), work out a weighted count of how many observations there are close to \\(x\\), where the weights are the kernel distribution’s density function evaluated at each observation. The idea is that observations closer to \\(x\\) should contribute more to the weighted count.\nThis is not something you would want to calculate by hand, but we are no longer in the 1960s, so we no longer need to do that. Here is the kernel-density-estimated smoothed histogram for the athletes’ weights:\n\n\nggplot(athletes, aes(x = Wt)) + geom_density()\n\n\n\n\nA nice smooth version of the histogram. By default, this uses a normal kernel distribution with a standard deviation chosen by a rule of thumb. The smoothness can be adjusted by using a value of adjust different from 1:\n\n\nggplot(athletes, aes(x = Wt)) + geom_density(adjust = 3)\n\n\n\n\nA value bigger than 1 estimates from a wider range of data, so the resulting density plot looks smoother.\n\n\nggplot(athletes, aes(x = Wt)) + geom_density(adjust = 0.3)\n\n\n\n\nA value of adjust less than 1 reacts more sharply to local features of the data, producing a less smooth graph.\nAnother feature of density plots is that you can “stack” several behind each other, to compare distributions. Let’s compare the weights of the male and female1 athletes. To do that, use a fill on the aes and specify the categorical variable there:\n\n\nggplot(athletes, aes(x = Wt, fill = Sex)) + geom_density()\n\n\n\n\nThis is not quite as we want: the upper tail of the weight distribution for the female athletes has disappeared behind the male ones, so we don’t know how high the female athletes’ weights go. To make it so that we can see both, we need to make the density curves partly transparent. In ggplot, you can make anything transparent by using a parameter alpha; a value of 1 means completely opaque (ie. like this), and a value of 0 means completely transparent (ie. invisible). So we can try this:\n\n\nggplot(athletes, aes(x = Wt, fill = Sex)) + geom_density(alpha = 0.5)\n\n\n\n\nThis makes it clearer that both distributions of weights have an upper tail, and that the female athletes’ weights to up to about 100 kg, heavier than the average of the male athlete weights. The athletes that are lighter in weight are all females, however.\nI’d have to say, though, that comparing distributions using density plots is easier with a relatively small number of distributions. Comparing all ten sports might be too much:\n\n\nggplot(athletes, aes(x = Wt, fill = Sport)) + geom_density(alpha = 0.5)\n\n\n\n\nIt’s more than a little difficult to distinguish all those colours, never mind to see where their density estimates are. When using fill, which colours the inside of something in ggplot, the colours of overlapping things also get mixed. An alternative is to use colour rather than fill, which colours the outside:\n\n\nggplot(athletes, aes(x = Wt, colour = Sport)) + geom_density(alpha = 0.5)\n\n\n\n\nThis time, it’s a little easier to see where each density goes, but it is still equally difficult to distinguish the ten colours from each other. For this many distributions, the boxplot is still a good way to compare them.\nIf we just look at, say, four of the sports, the density plot is more useful:\n\n\nthe_sports <- c(\"Gym\", \"Netball\", \"BBall\", \"Field\")\nathletes %>% \n  filter(Sport %in% the_sports) %>% \n  ggplot(aes(x = Wt, fill = Sport)) + geom_density(alpha = 0.5)\n\n\n\n\nThis shows that the gymnasts are the lightest weight, the netball players have a compact distribution of weights centred around 65 kg, and the basketball players and field athletes have a much greater spread of weights, with the field athletes being heavier overall. (It’s a little difficult to see the distribution of weights of basketball players other than by elimination, because the density estimate for basketball players is hidden behind the others except around weight 80 kg.)\nFinal thoughts\nFor a less statistically-educated audience, the density plot has less “baggage” than other plots like the boxplot; it is rather clearer where most of the values are, whether there is a greater or lesser spread, and what the shape looks like. It is also straightforward to see how distributions compare by making a density plot like the one above with the density estimates one behind the other. For a statistical audience, it is clear by looking at a boxplot how distributions compare, but boxplots have more baggage in that for a general audience, there needs to be discussion of median, quartiles and outliers in order to make sense of the plot. I was motivated to write this post because I did a “webinar” for my Toastmasters club using the Palmer Penguins data, and in preparing that, I found that density plots gave a clear picture of how the three species of penguin compared on the physical measurements in that dataset.\n\nFor this dataset, that means eligible to compete in athletic events for men and women.↩︎\n",
    "preview": "posts/2021-11-07-density-plots/density-plots_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2021-11-07T20:13:30-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-07-correcting-a-dataframe/",
    "title": "Correcting a dataframe",
    "description": "The tidyverse way.",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2021-04-26",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\nlibrary(tmaptools)\nlibrary(leaflet)\n\n\n\nIntroduction\nSo I had a dataframe today, in which I wanted to make some small corrections. Specifically, I had this one:\n\n\nmy_url <- \"http://ritsokiguess.site/datafiles/wisconsin.txt\"\nwisc <- read_table(my_url)\nwisc %>% select(location)\n\n\n# A tibble: 12 × 1\n   location     \n   <chr>        \n 1 Appleton     \n 2 Beloit       \n 3 Fort.Atkinson\n 4 Madison      \n 5 Marshfield   \n 6 Milwaukee    \n 7 Monroe       \n 8 Superior     \n 9 Wausau       \n10 Dubuque      \n11 St.Paul      \n12 Chicago      \n\nThese are mostly, but not all, cities in Wisconsin, and I want to draw them on a map. To do that, though, I need to affix their states to them, and I thought a good starting point was to start by pretending that they were all in Wisconsin, and then correct the ones that aren’t:\n\n\nwisc %>% select(location) %>% \n  mutate(state = \"WI\") -> wisc\nwisc\n\n\n# A tibble: 12 × 2\n   location      state\n   <chr>         <chr>\n 1 Appleton      WI   \n 2 Beloit        WI   \n 3 Fort.Atkinson WI   \n 4 Madison       WI   \n 5 Marshfield    WI   \n 6 Milwaukee     WI   \n 7 Monroe        WI   \n 8 Superior      WI   \n 9 Wausau        WI   \n10 Dubuque       WI   \n11 St.Paul       WI   \n12 Chicago       WI   \n\nThe last three cities are in the wrong state: Dubuque is in Iowa (IA), St. Paul in Minnesota (MN), and Chicago is in Illinois (IL). I know how to fix this in base R: I write something like\n\n\nwisc$state[12] <- \"IL\"\n\n\n\nbut how do you do this the Tidyverse way?\nA better way\nThe first step is to make a small dataframe with the cities that need to be corrected, and the states they are actually in:\n\n\ncorrections <- tribble(\n  ~location, ~state,\n  \"Dubuque\", \"IA\",\n  \"St.Paul\", \"MN\",\n  \"Chicago\", \"IL\"\n)\ncorrections\n\n\n# A tibble: 3 × 2\n  location state\n  <chr>    <chr>\n1 Dubuque  IA   \n2 St.Paul  MN   \n3 Chicago  IL   \n\nNote that the columns of this dataframe have the same names as the ones in the original dataframe wisc.\nSo, I was thinking, this is a lookup table (of a sort), and so joining this to wisc might yield something helpful. We want to look up locations and not match states, since we want to have these three cities have their correct state as a possibility. So what does this do?\n\n\nwisc %>% \n  left_join(corrections, by = \"location\")\n\n\n# A tibble: 12 × 3\n   location      state.x state.y\n   <chr>         <chr>   <chr>  \n 1 Appleton      WI      <NA>   \n 2 Beloit        WI      <NA>   \n 3 Fort.Atkinson WI      <NA>   \n 4 Madison       WI      <NA>   \n 5 Marshfield    WI      <NA>   \n 6 Milwaukee     WI      <NA>   \n 7 Monroe        WI      <NA>   \n 8 Superior      WI      <NA>   \n 9 Wausau        WI      <NA>   \n10 Dubuque       WI      IA     \n11 St.Paul       WI      MN     \n12 Chicago       WI      IL     \n\nNow, we have two states for each city. The first one is always Wisconsin, and the second one is usually missing, but where the state in state.y has a value, that is the true state of the city. So, the thought process is that the actual state should be:\nif state.y is not missing, use that\nelse, use the value in state.x.\nI had an idea that there was a function that would do exactly this, only I couldn’t remember its name, so I couldn’t really search for it. My first thought was na_if. What this does is every time it sees a certain value, it replaces it with NA. This, though, is the opposite way from what I wanted. So I looked at the See Also, and saw replace_na. This replaces NAs with a given value. Not quite right, but closer.\nIn the See Also for replace_na, I saw one more thing: coalesce, “replace NAs with values from other vectors”. Was that what I was thinking of? It was. The way it works is that you feed it several vectors, and the first one that is not missing gives its value to the result. Hence, what I needed was this:\n\n\nwisc %>% \n  left_join(corrections, by = \"location\") %>% \n  mutate(state=coalesce(state.y, state.x))\n\n\n# A tibble: 12 × 4\n   location      state.x state.y state\n   <chr>         <chr>   <chr>   <chr>\n 1 Appleton      WI      <NA>    WI   \n 2 Beloit        WI      <NA>    WI   \n 3 Fort.Atkinson WI      <NA>    WI   \n 4 Madison       WI      <NA>    WI   \n 5 Marshfield    WI      <NA>    WI   \n 6 Milwaukee     WI      <NA>    WI   \n 7 Monroe        WI      <NA>    WI   \n 8 Superior      WI      <NA>    WI   \n 9 Wausau        WI      <NA>    WI   \n10 Dubuque       WI      IA      IA   \n11 St.Paul       WI      MN      MN   \n12 Chicago       WI      IL      IL   \n\nWhere state.y has a value, it is used; if it’s missing, the value in state.x is used instead.\nThe best way\nI was quite pleased with myself for coming up with this, but I had missed the actual best way of doing this. In SQL, there is UPDATE, and what that does is to take a table of keys to look up and some new values for other columns to replace the ones in the original table. Because dplyr has a lot of things in common with SQL, it is perhaps no surprise that there is a rows_update, and for this job it is as simple as this:\n\n\nwisc %>% \n  rows_update(corrections) -> wisc\nwisc\n\n\n# A tibble: 12 × 2\n   location      state\n   <chr>         <chr>\n 1 Appleton      WI   \n 2 Beloit        WI   \n 3 Fort.Atkinson WI   \n 4 Madison       WI   \n 5 Marshfield    WI   \n 6 Milwaukee     WI   \n 7 Monroe        WI   \n 8 Superior      WI   \n 9 Wausau        WI   \n10 Dubuque       IA   \n11 St.Paul       MN   \n12 Chicago       IL   \n\nThe values to look up (the “keys”) are by default in the first column, which is where they are in corrections. If they had not been, I would have used a by in the same way as with a join.\nMind. Blown. (Well, my mind was, anyway.)\nGeocoding\nI said I wanted to draw a map with these cities on it. For that, I need to look up the longitude and latitude of these places, and for that, I need to glue the state onto the name of each city, to make sure I don’t look up the wrong one. It is perhaps easy to forget that unite is the cleanest way of doing this, particularly if you don’t want the individual columns any more:\n\n\nwisc %>% unite(where, c(location, state), sep = \" \") -> wisc\nwisc\n\n\n# A tibble: 12 × 1\n   where           \n   <chr>           \n 1 Appleton WI     \n 2 Beloit WI       \n 3 Fort.Atkinson WI\n 4 Madison WI      \n 5 Marshfield WI   \n 6 Milwaukee WI    \n 7 Monroe WI       \n 8 Superior WI     \n 9 Wausau WI       \n10 Dubuque IA      \n11 St.Paul MN      \n12 Chicago IL      \n\nThe function geocode_OSM from tmaptools will find the longitude and latitude of a place. It expects one place as input, not a vector of placenames, so we will work rowwise to geocode one at a time. (Using map from purrr is also an option.) The geocoder returns a list, which contains, buried a little deeply, the longitudes and latitudes:\n\n\nwisc %>% \n  rowwise() %>% \n  mutate(ll = list(geocode_OSM(where))) -> wisc\nwisc\n\n\n# A tibble: 12 × 2\n# Rowwise: \n   where            ll              \n   <chr>            <list>          \n 1 Appleton WI      <named list [3]>\n 2 Beloit WI        <named list [3]>\n 3 Fort.Atkinson WI <named list [3]>\n 4 Madison WI       <named list [3]>\n 5 Marshfield WI    <named list [3]>\n 6 Milwaukee WI     <named list [3]>\n 7 Monroe WI        <named list [3]>\n 8 Superior WI      <named list [3]>\n 9 Wausau WI        <named list [3]>\n10 Dubuque IA       <named list [3]>\n11 St.Paul MN       <named list [3]>\n12 Chicago IL       <named list [3]>\n\nThe column ll is a list-column, and the usual way to handle these is to unnest, but that isn’t quite right here:\n\n\nwisc %>% unnest(ll)\n\n\n# A tibble: 36 × 2\n   where            ll          \n   <chr>            <named list>\n 1 Appleton WI      <chr [1]>   \n 2 Appleton WI      <dbl [2]>   \n 3 Appleton WI      <bbox [4]>  \n 4 Beloit WI        <chr [1]>   \n 5 Beloit WI        <dbl [2]>   \n 6 Beloit WI        <bbox [4]>  \n 7 Fort.Atkinson WI <chr [1]>   \n 8 Fort.Atkinson WI <dbl [2]>   \n 9 Fort.Atkinson WI <bbox [4]>  \n10 Madison WI       <chr [1]>   \n# … with 26 more rows\n\nUnnesting a list of three things produces three rows for each city. It would make more sense to have the unnesting go to the right and produce a new column for each thing in the list. The new tidyr has a variant called unnest_wider that does this:\n\n\nwisc %>% \n  unnest_wider(ll)\n\n\n# A tibble: 12 × 4\n   where            query            coords    bbox      \n   <chr>            <chr>            <list>    <list>    \n 1 Appleton WI      Appleton WI      <dbl [2]> <bbox [4]>\n 2 Beloit WI        Beloit WI        <dbl [2]> <bbox [4]>\n 3 Fort.Atkinson WI Fort.Atkinson WI <dbl [2]> <bbox [4]>\n 4 Madison WI       Madison WI       <dbl [2]> <bbox [4]>\n 5 Marshfield WI    Marshfield WI    <dbl [2]> <bbox [4]>\n 6 Milwaukee WI     Milwaukee WI     <dbl [2]> <bbox [4]>\n 7 Monroe WI        Monroe WI        <dbl [2]> <bbox [4]>\n 8 Superior WI      Superior WI      <dbl [2]> <bbox [4]>\n 9 Wausau WI        Wausau WI        <dbl [2]> <bbox [4]>\n10 Dubuque IA       Dubuque IA       <dbl [2]> <bbox [4]>\n11 St.Paul MN       St.Paul MN       <dbl [2]> <bbox [4]>\n12 Chicago IL       Chicago IL       <dbl [2]> <bbox [4]>\n\nThe longitudes and latitudes we want are still hidden in a list-column, the one called coords, so with luck, if we unnest that wider as well, we should be in business:\n\n\nwisc %>% \n  unnest_wider(ll) %>% \n  unnest_wider(coords) -> wisc\nwisc\n\n\n# A tibble: 12 × 5\n   where            query                x     y bbox      \n   <chr>            <chr>            <dbl> <dbl> <list>    \n 1 Appleton WI      Appleton WI      -88.4  44.3 <bbox [4]>\n 2 Beloit WI        Beloit WI        -89.0  42.5 <bbox [4]>\n 3 Fort.Atkinson WI Fort.Atkinson WI -88.8  42.9 <bbox [4]>\n 4 Madison WI       Madison WI       -89.4  43.1 <bbox [4]>\n 5 Marshfield WI    Marshfield WI    -90.2  44.7 <bbox [4]>\n 6 Milwaukee WI     Milwaukee WI     -87.9  43.0 <bbox [4]>\n 7 Monroe WI        Monroe WI        -89.6  42.6 <bbox [4]>\n 8 Superior WI      Superior WI      -92.1  46.6 <bbox [4]>\n 9 Wausau WI        Wausau WI        -89.6  45.0 <bbox [4]>\n10 Dubuque IA       Dubuque IA       -90.7  42.5 <bbox [4]>\n11 St.Paul MN       St.Paul MN       -93.1  44.9 <bbox [4]>\n12 Chicago IL       Chicago IL       -87.6  41.9 <bbox [4]>\n\nAnd now we are. x contains the longitudes (negative for degrees west), and y the latitudes (positive for degrees north).\nMaking a map with these on them\nThe most enjoyable way to make a map in R is to use the leaflet package. Making a map is a three-step process:\nleaflet() with the name of the dataframe\naddTiles() to get map tiles to draw the map with\nadd some kind of markers to show where the points are. I use circle markers here; there are also markers (from addMarkers) that look like Google map pins. Here also you associate the longs and lats with the columns they are in in your dataframe:\n\n\nleaflet(data = wisc) %>% \n  addTiles() %>% \n  addCircleMarkers(lng = ~x, lat = ~y) \n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircleMarkers\",\"args\":[[44.2613967,42.5083272,42.9288944,43.074761,44.6688524,43.0349931,42.6018298,46.623324,44.9596017,42.5006217,44.9497487,41.8755616],[-88.4069744,-89.031784,-88.8370509,-89.3837613,-90.1717987,-87.922497,-89.6392396,-92.117435,-89.6298239,-90.6647967,-93.0931028,-87.6244212],10,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2},null,null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[41.8755616,46.623324],\"lng\":[-93.0931028,-87.6244212]}},\"evals\":[],\"jsHooks\":[]}\nThe nice thing about Leaflet maps is that you can zoom, pan and generally move about in them. For example, you can zoom in to find out which city each circle represents.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-07T21:32:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-07-sampling-locations-in-a-city/",
    "title": "Sampling locations in a city",
    "description": "with the aim of getting an aerial map of that location.",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2020-10-10",
    "categories": [],
    "contents": "\nIntroduction\nDo you follow @londonmapbot on Twitter? You should. Every so often a satellite photo is posted of somewhere in London (the one in England), with the implied invitation to guess where it is. Along with the tweet is a link to openstreetmap, and if you click on it, it gives you a map of where the photo is, so you can see whether your guess was right. Or, if you’re me, you look at the latitude and longitude in the link, and figure out roughly where in the city it is. My strategy is to note that Oxford Circus, in the centre of London, is at about 51.5 north and 0.15 west, and work from there.1\nMatt Dray, who is behind @londonmapbot, selects random points in a rectangle that goes as far in each compass direction as the M25 goes. (This motorway surrounds London in something like a circle, and is often taken as a definition of what is considered to be London; if outside, not in London. There is a surprising amount of countryside inside the M25.)\nLondon has the advantage of being roughly a rectangle aligned north-south and east-west, and is therefore easy to sample from. I have been thinking about doing something similar for my home city Toronto, but I ran into an immediate problem:\nToronto with boundaryToronto is not nicely aligned north-south and east-west, and so if you sample from a rectangle enclosing it, this is what will happen:\nrandomly sampled points from rectangle surrounding TorontoYou get some points inside the city, but you will also get a number of points in Vaughan or Mississauga or Pickering or Lake Ontario! How to eliminate the ones I don’t want?\nSampling from a region\nLet’s load some packages:\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sp)\n\n\n\nI had this vague idea that it would be possible to decide if a sampled point was inside a polygon or not. So I figured I would start by defining the boundary of Toronto as a collection of straight lines joining points, at least approximately. The northern boundary of Toronto is Steeles Avenue, all the way across, and that is a straight line, but the southern boundary is Lake Ontario, and the western and eastern boundaries are a mixture of streets and rivers, so I tried to pick points which, when joined by straight lines, enclosed all of Toronto without too much extra. This is what I came up with:\n\n\nboundary <- tribble(\n  ~where, ~lat, ~long,\n\"Steeles @ 427\", 43.75, -79.639,\n\"Steeles @ Pickering Townline\", 43.855, -79.17,\n\"Twyn Rivers @ Rouge River\", 43.815, -79.15,\n\"Rouge Beach\", 43.795, -79.115,\n\"Tommy Thompson Park\", 43.61, -79.33,\n\"Gibraltar Point\", 43.61, -79.39,\n\"Sunnyside Beach\", 43.635, -79.45,\n\"Cliff Lumsden Park\", 43.59, -79.50,\n\"Marie Curtis Park\", 43.58, -79.54,\n\"Rathburn @ Mill\", 43.645, -79.59,\n\"Etobicoke Creek @ Eglinton\", 43.645, -79.61,\n\"Eglinton @ Renforth\", 43.665, -79.59,\n\"Steeles @ 427\", 43.75, -79.639,\n)\nboundary\n\n\n# A tibble: 13 × 3\n   where                          lat  long\n   <chr>                        <dbl> <dbl>\n 1 Steeles @ 427                 43.8 -79.6\n 2 Steeles @ Pickering Townline  43.9 -79.2\n 3 Twyn Rivers @ Rouge River     43.8 -79.2\n 4 Rouge Beach                   43.8 -79.1\n 5 Tommy Thompson Park           43.6 -79.3\n 6 Gibraltar Point               43.6 -79.4\n 7 Sunnyside Beach               43.6 -79.4\n 8 Cliff Lumsden Park            43.6 -79.5\n 9 Marie Curtis Park             43.6 -79.5\n10 Rathburn @ Mill               43.6 -79.6\n11 Etobicoke Creek @ Eglinton    43.6 -79.6\n12 Eglinton @ Renforth           43.7 -79.6\n13 Steeles @ 427                 43.8 -79.6\n\nI kind of had the idea that you could determine whether a point was inside a polygon or not. The idea turns out to be this: you draw a line to the right from your point; if it crosses the boundary of the polygon an odd number of times, it’s inside, and if an even number of times, it’s outside. So is there something like this in R? Yes: this function in the sp package.2\nSo now I could generate some points in the enclosing rectangle and see whether they were inside or outside the city, like this:\n\n\nset.seed(457299)\nn_point <- 20\ntibble(lat = runif(n_point, min(boundary$lat), max(boundary$lat)),\n       long = runif(n_point, min(boundary$long), max(boundary$long))) -> d\nd %>% mutate(inside = point.in.polygon(d$long, d$lat, boundary$long, boundary$lat)) %>% \n  mutate(colour = ifelse(inside == 1, \"blue\", \"red\")) -> d\nd\n\n\n# A tibble: 20 × 4\n     lat  long inside colour\n   <dbl> <dbl>  <int> <chr> \n 1  43.8 -79.6      0 red   \n 2  43.6 -79.5      0 red   \n 3  43.6 -79.6      1 blue  \n 4  43.7 -79.3      1 blue  \n 5  43.7 -79.2      0 red   \n 6  43.8 -79.3      1 blue  \n 7  43.6 -79.2      0 red   \n 8  43.6 -79.2      0 red   \n 9  43.7 -79.5      1 blue  \n10  43.8 -79.2      1 blue  \n11  43.8 -79.4      1 blue  \n12  43.7 -79.5      1 blue  \n13  43.6 -79.1      0 red   \n14  43.7 -79.4      1 blue  \n15  43.8 -79.4      1 blue  \n16  43.8 -79.2      1 blue  \n17  43.7 -79.3      1 blue  \n18  43.7 -79.1      0 red   \n19  43.8 -79.6      0 red   \n20  43.7 -79.2      1 blue  \n\nThe function point.in.polygon returns a 1 if the point is inside the polygon (city boundary) and a 0 if outside.3\nI added a column colour to plot the inside and outside points in different colours on a map, which we do next. The leaflet package is much the easiest way to do this:\n\n\nleaflet(d) %>% \n  addTiles() %>% \n  addCircleMarkers(color = d$colour) %>% \n    addPolygons(boundary$long, boundary$lat)\n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircleMarkers\",\"args\":[[43.8405856347538,43.6142636974005,43.6426254201052,43.7333733423392,43.6883476533822,43.7869079166744,43.6465799339535,43.6396968865662,43.740718059208,43.8079513767513,43.7892276994954,43.6821760909643,43.6186219538923,43.7004526383779,43.7960451051954,43.7757609975478,43.7179439867509,43.6734056267957,43.8174444213754,43.7353308267088],[-79.6133690229235,-79.4667217004932,-79.5794000967983,-79.3068632841418,-79.1685870058518,-79.3238925372232,-79.2202741911337,-79.2137170310309,-79.5196319680633,-79.2255765117444,-79.3659771945663,-79.5367257098705,-79.1490086372094,-79.4227933171159,-79.4004283973714,-79.2079592188233,-79.321934687146,-79.1151048495993,-79.6166347121717,-79.1885206499314],10,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":[\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\"],\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":[\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\"],\"fillOpacity\":0.2},null,null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addPolygons\",\"args\":[[[[{\"lng\":[-79.639,-79.17,-79.15,-79.115,-79.33,-79.39,-79.45,-79.5,-79.54,-79.59,-79.61,-79.59,-79.639],\"lat\":[43.75,43.855,43.815,43.795,43.61,43.61,43.635,43.59,43.58,43.645,43.645,43.665,43.75]}]]],null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2,\"smoothFactor\":1,\"noClip\":false},null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[43.58,43.855],\"lng\":[-79.639,-79.115]}},\"evals\":[],\"jsHooks\":[]}\nThe polygons come from a different dataframe, so I need to specify that in addPolygons. Leaflet is clever enough to figure out which is longitude and which latitude (there are several possibilities it will understand).\nThis one seems to have classified the points more or less correctly. The bottom left red circle is just in the lake, though it looks as if one of the three rightmost blue circles is in the lake also. Oops. The way to test this is to generate several sets of random points, test the ones near the boundary, and if they were classified wrongly, tweak the boundary points and try again. The coastline around the Scarborough Bluffs is not as straight as I was hoping.\nMapbox\nMatt Dray’s blog post gives a nice clear explanation of how to set up MapBox to return you a satellite image of a lat and long you feed it. What you need is a Mapbox API key. A good place to save this is in your .Renviron, and edit_r_environ from usethis is a good way to get at that. Then you use this key to construct a URL that will return you an image of that point.\nLet’s grab one of those sampled points that actually is in Toronto:\n\n\nd %>% filter(inside == 1) %>% slice(1) -> d1\nd1\n\n\n# A tibble: 1 × 4\n    lat  long inside colour\n  <dbl> <dbl>  <int> <chr> \n1  43.6 -79.6      1 blue  \n\nand then I get my API key and use it to make a URL for an image at this point:\n\n\nmapbox_token <- Sys.getenv(\"MAPBOX_TOKEN\")\nurl <- str_c(\"https://api.mapbox.com/styles/v1/mapbox/satellite-v9/static/\",\n             d1$long,\n             \",\",\n             d1$lat,\n             \",15,0/600x400?access_token=\",\n             mapbox_token)\n\n\n\nI’m not showing you the actual URL, since it contains my key! The last-but-one line contains the zoom (15) and the size of the image (600 by 400). These are slightly more zoomed out and bigger than the values Matt uses. (I wanted to have a wider area to make it easier to guess.)\nThen download this and save it somewhere:\n\n\nwhere <- \"img.png\"\ndownload.file(url, where)\n\n\n\nand display it:\nsatellite image of somewhere in TorontoI don’t recognize that, so I’ll fire up leaflet again:\n\n\nleaflet(d1) %>% \n  addTiles() %>% \n  addCircleMarkers() \n\n\n\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircleMarkers\",\"args\":[43.6426254201052,-79.5794000967983,10,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2},null,null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[43.6426254201052,43.6426254201052],\"lng\":[-79.5794000967983,-79.5794000967983]}},\"evals\":[],\"jsHooks\":[]}\nIt’s the bit of Toronto that’s almost in Mississauga. The boundary is Etobicoke Creek, at the bottom left of the image.\nReferences\nHow to determine if point inside polygon\npoint.in.polygon function documentation\nMatt Dray blog post on londonmapbot\n\nLondon extends roughly between latitude 51.2 and 51.7 degrees, and between longitude 0.25 degrees east and 0.5 west. Knowing this enables you to place a location in London from its lat and long.↩︎\nHaving had a bad experience with rgdal earlier, I was afraid that sp would be a pain to install, but there was no problem at all.↩︎\nIt also returns a 2 if the point is on an edge of the polygon and a 3 if at a vertex.↩︎\n",
    "preview": "posts/2021-11-07-sampling-locations-in-a-city/Screenshot_2020-10-10_12-40-39.png",
    "last_modified": "2021-11-07T21:16:35-05:00",
    "input_file": {},
    "preview_width": 617,
    "preview_height": 431
  },
  {
    "path": "posts/2020-07-09-another-tidying-problem/",
    "title": "Another tidying problem",
    "description": "that ends up with a matched pairs test after tidying.",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2020-07-09",
    "categories": [],
    "contents": "\nIntroduction\nSome cars have a computer that records gas mileage since the last time the computer was reset. A driver is concerned that the computer on their car is not as accurate as it might be, so they keep an old-fashioned notebook and record the miles driven since the last fillup, and the amount of gas filled up, and use that to compute the miles per gallon. They also record what the car’s computer says the miles per gallon was.\nIs there a systematic difference between the computer’s values and the driver’s? If so, which way does it go?\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nThe data\nThe driver’s notebook has small pages, so the data look like this:\nFillup     1    2    3    4    5\nComputer 41.5 50.7 36.6 37.3 34.2\nDriver   36.5 44.2 37.2 35.6 30.5\nFillup     6    7    8    9   10\nComputer 45.0 48.0 43.2 47.7 42.2\nDriver   40.5 40.0 41.0 42.8 39.2\nFillup    11   12   13   14   15\nComputer 43.2 44.6 48.4 46.4 46.8\nDriver   38.8 44.5 45.4 45.3 45.7\nFillup    16   17   18   19   20\nComputer 39.2 37.3 43.5 44.3 43.3\nDriver   34.2 35.2 39.8 44.9 47.5\nThis is not very close to tidy. There are three variables: the fillup number (identification), the computer’s miles-per-gallon value, and the driver’s. These should be in columns, not rows. Also, there are really four sets of rows, because of the way the data was recorded. How are we going to make this tidy?\nMaking it tidy\nAs ever, we take this one step at a time, building a pipeline as we go: we see what each step produces before figuring out what to do next.\nThe first thing is to read the data in; these are aligned columns, so read_table is the thing. Also, there are no column headers, so we have to say that as well:\n\n\nmy_url <- \"https://raw.githubusercontent.com/nxskok/nxskok.github.io/master/gas-mileage.txt\"\ngas <- read_table(my_url, col_names = FALSE)\ngas\n\n\n# A tibble: 12 × 6\n   X1          X2    X3    X4    X5    X6\n   <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Fillup     1     2     3     4     5  \n 2 Computer  41.5  50.7  36.6  37.3  34.2\n 3 Driver    36.5  44.2  37.2  35.6  30.5\n 4 Fillup     6     7     8     9    10  \n 5 Computer  45    48    43.2  47.7  42.2\n 6 Driver    40.5  40    41    42.8  39.2\n 7 Fillup    11    12    13    14    15  \n 8 Computer  43.2  44.6  48.4  46.4  46.8\n 9 Driver    38.8  44.5  45.4  45.3  45.7\n10 Fillup    16    17    18    19    20  \n11 Computer  39.2  37.3  43.5  44.3  43.3\n12 Driver    34.2  35.2  39.8  44.9  47.5\n\nLonger first\nI usually find it easier to make the dataframe longer first, and then figure out what to do next. Here, that means putting all the data values in one column, and having a column of variable names indicating what each variable is a value of, thus:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\")\n\n\n# A tibble: 60 × 3\n   X1       var_name var_value\n   <chr>    <chr>        <dbl>\n 1 Fillup   X2             1  \n 2 Fillup   X3             2  \n 3 Fillup   X4             3  \n 4 Fillup   X5             4  \n 5 Fillup   X6             5  \n 6 Computer X2            41.5\n 7 Computer X3            50.7\n 8 Computer X4            36.6\n 9 Computer X5            37.3\n10 Computer X6            34.2\n# … with 50 more rows\n\nThe things in X1 are our column-names-to-be, and the values that go with them are in var_value. var_name has mostly served its purpose; these are the original columns in the data file, which we don’t need any more. So now, we make this wider, right?\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value)  \n\n\n# A tibble: 5 × 4\n  var_name Fillup    Computer  Driver   \n  <chr>    <list>    <list>    <list>   \n1 X2       <dbl [4]> <dbl [4]> <dbl [4]>\n2 X3       <dbl [4]> <dbl [4]> <dbl [4]>\n3 X4       <dbl [4]> <dbl [4]> <dbl [4]>\n4 X5       <dbl [4]> <dbl [4]> <dbl [4]>\n5 X6       <dbl [4]> <dbl [4]> <dbl [4]>\n\nOh. How did we get list-columns?\nThe answer is that pivot_wider needs to know which column each var_value is going to, but also which row. The way it decides about rows is to look at all combinations of things in the other columns, the ones not involved in the pivot_wider. The only one of those here is var_name, so each value goes in the column according to its value in X1, and the row according to its value in var_name. For example, the value 41.5 in row 6 of the longer dataframe goes into the column labelled Computer and the row labelled X2. But if you scroll down the longer dataframe, you’ll find there are four data values with the Computer-X2 combination, so pivot_wider collects them together into one cell of the output dataframe.\nThis is what the warning is about.\nspread handled this much less gracefully:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  spread(X1, var_value)  \n\n\nError: Each row of output must be identified by a unique combination of keys.\nKeys are shared for 60 rows:\n* 6, 21, 36, 51\n* 7, 22, 37, 52\n* 8, 23, 38, 53\n* 9, 24, 39, 54\n* 10, 25, 40, 55\n* 11, 26, 41, 56\n* 12, 27, 42, 57\n* 13, 28, 43, 58\n* 14, 29, 44, 59\n* 15, 30, 45, 60\n* 1, 16, 31, 46\n* 2, 17, 32, 47\n* 3, 18, 33, 48\n* 4, 19, 34, 49\n* 5, 20, 35, 50\n\nIt required a unique combination of values for the other variables in the dataframe, which in our case we have not got.\nAll right, back to this:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value)  \n\n\n# A tibble: 5 × 4\n  var_name Fillup    Computer  Driver   \n  <chr>    <list>    <list>    <list>   \n1 X2       <dbl [4]> <dbl [4]> <dbl [4]>\n2 X3       <dbl [4]> <dbl [4]> <dbl [4]>\n3 X4       <dbl [4]> <dbl [4]> <dbl [4]>\n4 X5       <dbl [4]> <dbl [4]> <dbl [4]>\n5 X6       <dbl [4]> <dbl [4]> <dbl [4]>\n\nThere is a mindless way to go on from here, and a thoughtful way.\nThe mindless way to handle unwanted list-columns is to throw an unnest at the problem and see what happens:\n\n\ngas %>% pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value)  %>% \n  unnest()\n\n\nWarning: Values are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = length` to identify where the duplicates arise\n* Use `values_fn = {summary_fun}` to summarise duplicates\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(Fillup, Computer, Driver)`\n# A tibble: 20 × 4\n   var_name Fillup Computer Driver\n   <chr>     <dbl>    <dbl>  <dbl>\n 1 X2            1     41.5   36.5\n 2 X2            6     45     40.5\n 3 X2           11     43.2   38.8\n 4 X2           16     39.2   34.2\n 5 X3            2     50.7   44.2\n 6 X3            7     48     40  \n 7 X3           12     44.6   44.5\n 8 X3           17     37.3   35.2\n 9 X4            3     36.6   37.2\n10 X4            8     43.2   41  \n11 X4           13     48.4   45.4\n12 X4           18     43.5   39.8\n13 X5            4     37.3   35.6\n14 X5            9     47.7   42.8\n15 X5           14     46.4   45.3\n16 X5           19     44.3   44.9\n17 X6            5     34.2   30.5\n18 X6           10     42.2   39.2\n19 X6           15     46.8   45.7\n20 X6           20     43.3   47.5\n\nThis has worked.1 The fillup numbers have come out in the wrong order, but that’s probably not a problem. It would also work if you had a different number of observations on each row of the original data file, as long as you had a fillup number, a computer value and a driver value for each one.\nThe thoughtful way to go is to organize it so that each row will have a unique combination of columns that are left. A way to do that is to note that the original data file has four “blocks” of five observations each:\n\n\ngas\n\n\n# A tibble: 12 × 6\n   X1          X2    X3    X4    X5    X6\n   <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Fillup     1     2     3     4     5  \n 2 Computer  41.5  50.7  36.6  37.3  34.2\n 3 Driver    36.5  44.2  37.2  35.6  30.5\n 4 Fillup     6     7     8     9    10  \n 5 Computer  45    48    43.2  47.7  42.2\n 6 Driver    40.5  40    41    42.8  39.2\n 7 Fillup    11    12    13    14    15  \n 8 Computer  43.2  44.6  48.4  46.4  46.8\n 9 Driver    38.8  44.5  45.4  45.3  45.7\n10 Fillup    16    17    18    19    20  \n11 Computer  39.2  37.3  43.5  44.3  43.3\n12 Driver    34.2  35.2  39.8  44.9  47.5\n\nEach set of three rows is one block. So if we number the blocks, each observation of Fillup, Computer, and Driver will have an X-something column that it comes from and a block, and this combination will be unique.\nYou could create the block column by hand easily enough, or note that each block starts with a row called Fillup and use this idea:\n\n\ngas %>% mutate(block = cumsum(X1==\"Fillup\"))\n\n\n# A tibble: 12 × 7\n   X1          X2    X3    X4    X5    X6 block\n   <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <int>\n 1 Fillup     1     2     3     4     5       1\n 2 Computer  41.5  50.7  36.6  37.3  34.2     1\n 3 Driver    36.5  44.2  37.2  35.6  30.5     1\n 4 Fillup     6     7     8     9    10       2\n 5 Computer  45    48    43.2  47.7  42.2     2\n 6 Driver    40.5  40    41    42.8  39.2     2\n 7 Fillup    11    12    13    14    15       3\n 8 Computer  43.2  44.6  48.4  46.4  46.8     3\n 9 Driver    38.8  44.5  45.4  45.3  45.7     3\n10 Fillup    16    17    18    19    20       4\n11 Computer  39.2  37.3  43.5  44.3  43.3     4\n12 Driver    34.2  35.2  39.8  44.9  47.5     4\n\nThis works because X1==\"Fillup\" is either true or false. cumsum takes cumulative sums; that is, the sum of all the values in the column down to and including the one you’re looking at. It requires numeric input, though, so it turns the logical values into 1 for TRUE and 0 for FALSE and adds those up. (This is the same thing that as.numeric does.) The idea is that the value of block gets bumped by one every time you hit a Fillup line.\nThen pivot-longer as before:\n\n\ngas %>% mutate(block = cumsum(X1==\"Fillup\")) %>% \n  pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\")\n\n\n# A tibble: 60 × 4\n   X1       block var_name var_value\n   <chr>    <int> <chr>        <dbl>\n 1 Fillup       1 X2             1  \n 2 Fillup       1 X3             2  \n 3 Fillup       1 X4             3  \n 4 Fillup       1 X5             4  \n 5 Fillup       1 X6             5  \n 6 Computer     1 X2            41.5\n 7 Computer     1 X3            50.7\n 8 Computer     1 X4            36.6\n 9 Computer     1 X5            37.3\n10 Computer     1 X6            34.2\n# … with 50 more rows\n\nand now you can check that the var_name - block combinations are unique for each value in X1, so pivoting wider should work smoothly now:\n\n\n(gas %>% mutate(block = cumsum(X1==\"Fillup\")) %>% \n  pivot_longer(X2:X6, names_to = \"var_name\", values_to = \"var_value\") %>% \n  pivot_wider(names_from = X1, values_from = var_value) -> gas1)\n\n\n# A tibble: 20 × 5\n   block var_name Fillup Computer Driver\n   <int> <chr>     <dbl>    <dbl>  <dbl>\n 1     1 X2            1     41.5   36.5\n 2     1 X3            2     50.7   44.2\n 3     1 X4            3     36.6   37.2\n 4     1 X5            4     37.3   35.6\n 5     1 X6            5     34.2   30.5\n 6     2 X2            6     45     40.5\n 7     2 X3            7     48     40  \n 8     2 X4            8     43.2   41  \n 9     2 X5            9     47.7   42.8\n10     2 X6           10     42.2   39.2\n11     3 X2           11     43.2   38.8\n12     3 X3           12     44.6   44.5\n13     3 X4           13     48.4   45.4\n14     3 X5           14     46.4   45.3\n15     3 X6           15     46.8   45.7\n16     4 X2           16     39.2   34.2\n17     4 X3           17     37.3   35.2\n18     4 X4           18     43.5   39.8\n19     4 X5           19     44.3   44.9\n20     4 X6           20     43.3   47.5\n\nand so it does.\nSometimes a pivot_longer followed by a pivot_wider can be turned into a single pivot_longer with options (see the pivoting vignette for examples), but this appears not to be one of those.\nComparing the driver and the computer\nNow that we have tidy data, we can do an analysis. These are matched-pair data (one Computer and one Driver measurement), so a sensible graph would be of the differences, a histogram, say:\n\n\ngas1 %>% mutate(diff = Computer - Driver) %>% \n  ggplot(aes(x=diff)) + geom_histogram(bins=6)\n\n\n\n\nThere is only one observation where the driver’s measurement is much bigger than the computer’s; otherwise, there is not much to choose or the computer’s measurement is bigger. Is this something that would generalize to “all measurements”, presumably all measurements at fillup by this driver and this computer? The differences are not badly non-normal, so a \\(t\\)-test should be fine:\n\n\nwith(gas1, t.test(Computer, Driver, paired = TRUE))\n\n\n\n    Paired t-test\n\ndata:  Computer and Driver\nt = 4.358, df = 19, p-value = 0.0003386\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1.418847 4.041153\nsample estimates:\nmean of the differences \n                   2.73 \n\nIt is. The computer’s mean measurement is estimated to be between about 1.4 and 4.0 miles per gallon larger than the driver’s.\nReferences\nData from here, exercise 7.35.\nNaming of parts\nPivoting vignette from tidyr\n\nI did get away with using unnest the old-fashioned way, though. What I should have done is given below the second warning.↩︎\n",
    "preview": "posts/2020-07-09-another-tidying-problem/another-tidying-problem_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2021-11-07T20:06:26-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-14-understanding-the-result-of-a-chi-square-test/",
    "title": "Understanding the result of a chi-square test",
    "description": "Going beyond the chi-square statistic and its P-value",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2020-03-14",
    "categories": [],
    "contents": "\nIntroduction\nA chisquare test can be used for assessing whether there is association between two categorical variables. The problem it has is that knowing that an association exists is only part of the story; we want to know what is making the association happen. This is the same kind of thing that happens with analysis of variance: a significant \\(F\\)-test indicates that the group means are not all the same, but not which ones are different.\nRecently I discovered that R’s chisq.test has something that will help in understanding this.\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nwhich I always seem to need for something.\nExample\nHow do males and females differ in their choice of eyewear (glasses, contacts, neither), if at all? Some data (frequencies):\n\n\neyewear <- tribble(\n  ~gender, ~contacts, ~glasses, ~none,\n  \"female\", 121, 32, 129,\n  \"male\", 42, 37, 85\n)\neyewear\n\n\n# A tibble: 2 × 4\n  gender contacts glasses  none\n  <chr>     <dbl>   <dbl> <dbl>\n1 female      121      32   129\n2 male         42      37    85\n\nIt is a little difficult to compare since there are fewer males than females here, but we might suspect that males proportionately are more likely to wear glasses and less likely to wear contacts than females.\nDoes the data support an association at all?\n\n\neyewear %>% select(-gender) %>% chisq.test() -> z\nz\n\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 17.718, df = 2, p-value = 0.0001421\n\nThere is indeed an association.\nCoding note: normally chisq.test accepts as input a matrix (eg. output from table), but it also accepts a data frame as long as all the columns are frequencies. So I had to remove the gender column first.1\nSo, what kind of association? chisq.test has, as part of its output, residuals. Maybe you remember calculating these tests by hand, and have, lurking in the back of your mind somewhere, “observed minus expected, squared, divide by expected”. There is one of these for each cell, and you add them up to get the test statistic. The “Pearson residuals” in a chi-squared table are the signed square roots of these, where the sign is negative if observed is less than expected:\n\n\neyewear\n\n\n# A tibble: 2 × 4\n  gender contacts glasses  none\n  <chr>     <dbl>   <dbl> <dbl>\n1 female      121      32   129\n2 male         42      37    85\n\nz$residuals\n\n\n      contacts   glasses       none\n[1,]  1.766868 -1.760419 -0.5424069\n[2,] -2.316898  2.308440  0.7112591\n\nThe largest (in size) residuals make the biggest contribution to the chi-squared test statistic, so these are the ones where observed and expected are farthest apart. Hence, here, fewer males wear contacts and more males wear glasses compared to what you would expect if there were no association between gender and eyewear.\nI am not quite being sexist here: the male and female frequencies are equally far away from the expected in absolute terms:\n\n\neyewear\n\n\n# A tibble: 2 × 4\n  gender contacts glasses  none\n  <chr>     <dbl>   <dbl> <dbl>\n1 female      121      32   129\n2 male         42      37    85\n\nz$expected\n\n\n      contacts glasses      none\n[1,] 103.06278 43.6278 135.30942\n[2,]  59.93722 25.3722  78.69058\n\nbut the contribution to the test statistic is more for the males because there are fewer of them altogether.\n\nThis behaviour undoubtedly comes from the days when matrices had row names which didn’t count as a column.↩︎\n",
    "preview": {},
    "last_modified": "2021-11-12T20:35:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-08-two-header-rows-and-other-spreadsheets/",
    "title": "Two header rows and other spreadsheets",
    "description": "Tidying data arranged in odd ways",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2019-12-01",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nIntroduction\nA friend tells you that they are trying to find out which combination of detergent and temperature gets the most dirt off their laundry. They send you a spreadsheet that looks like this:\nData in spreadsheetThe first row is the name of the detergent (only named once), and the second row is the washing temperature. Below that is the amount of dirt removed from each of four loads of laundry washed under those conditions. (You know that your friend knows something about statistics and would have been careful to randomize loads of laundry to treatments.)\nThis is not going to be very helpful to you because it has two header rows. Fortunately Alison Hill has a blog post on almost this thing, which we can steal. In hers, the first row was variable names and the second was variable descriptions (which she used to make a data dictionary). Here, though, the column names need to be made out of bits of both rows.\nMaking column names\nThe strategy is the same as Alison used (so I’m claiming very little originality here): read the header lines and make column names out of them, then read the rest of the data with the column names that we made.\nYour friend supplied you with a .csv file (they do have some training, after all):\n\n\nmy_file <- \"https://raw.githubusercontent.com/nxskok/nxskok.github.io/master/static/detergent.csv\"\nheaders <- read_csv(my_file, col_names=F, n_max=2)\nheaders\n\n\n# A tibble: 2 × 6\n  X1    X2    X3    X4    X5    X6   \n  <chr> <chr> <chr> <chr> <chr> <chr>\n1 Super <NA>  <NA>  Best  <NA>  <NA> \n2 Cold  Warm  Hot   Cold  Warm  Hot  \n\nCouple of things here: we want read_csv to supply some dummy column names, and we want to read only two rows.\nTo use this, we want to construct some column names, but to do this it will be much easier if we have six rows and a few columns. For me, this is an everything-looks-like-a-nail moment, and I reach for gather, and then stop myself just in time to use pivot_longer instead. To keep things straight, I’m going to make a new column first so that I know what is what, and then use the default column names name and value in pivot_longer until I figure out what I’m doing:\n\n\nheaders %>% mutate(what=c(\"detergent\", \"temperature\")) %>% \n  pivot_longer(-what)\n\n\n# A tibble: 12 × 3\n   what        name  value\n   <chr>       <chr> <chr>\n 1 detergent   X1    Super\n 2 detergent   X2    <NA> \n 3 detergent   X3    <NA> \n 4 detergent   X4    Best \n 5 detergent   X5    <NA> \n 6 detergent   X6    <NA> \n 7 temperature X1    Cold \n 8 temperature X2    Warm \n 9 temperature X3    Hot  \n10 temperature X4    Cold \n11 temperature X5    Warm \n12 temperature X6    Hot  \n\nSo now it looks as if I want to pivot_wider that column what, getting the values from value. (At this point, I feel a nagging doubt that I can do this with one pivot_longer, but anyway):1\n\n\nheaders %>% mutate(what=c(\"detergent\", \"temperature\")) %>% \n  pivot_longer(-what) %>% \n  pivot_wider(names_from=what, values_from=value) -> d1\nd1\n\n\n# A tibble: 6 × 3\n  name  detergent temperature\n  <chr> <chr>     <chr>      \n1 X1    Super     Cold       \n2 X2    <NA>      Warm       \n3 X3    <NA>      Hot        \n4 X4    Best      Cold       \n5 X5    <NA>      Warm       \n6 X6    <NA>      Hot        \n\nMuch better. Next, I need to fill those missing values in detergent, and then I glue those two things together to make my column names:\n\n\nd1 %>% fill(detergent) %>% \n  mutate(mycol=str_c(detergent, temperature, sep=\"_\")) -> d2\nd2\n\n\n# A tibble: 6 × 4\n  name  detergent temperature mycol     \n  <chr> <chr>     <chr>       <chr>     \n1 X1    Super     Cold        Super_Cold\n2 X2    Super     Warm        Super_Warm\n3 X3    Super     Hot         Super_Hot \n4 X4    Best      Cold        Best_Cold \n5 X5    Best      Warm        Best_Warm \n6 X6    Best      Hot         Best_Hot  \n\nand then grab my desired column names as a vector:\n\n\nd2 %>% pull(mycol) -> my_col_names\n\n\n\nConstructing the data frame with the rest of the data\nNow we need to read the actual data, which means skipping the first two rows, and while doing so, use the column names we made as column names for the data frame (Alison’s idea again):\n\n\nlaundry <- read_csv(my_file, skip=2, col_names=my_col_names)\nlaundry\n\n\n# A tibble: 4 × 6\n  Super_Cold Super_Warm Super_Hot Best_Cold Best_Warm Best_Hot\n       <dbl>      <dbl>     <dbl>     <dbl>     <dbl>    <dbl>\n1          4          7        10         6        13       12\n2          5          9        12         6        15       13\n3          6          8        11         4        12       10\n4          5         12         9         4        12       13\n\nLooking good so far.\nWe need to make this longer to do anything useful with it. Each column name encodes two things: a detergent name and a temperature, and this can be made longer in one shot by using two things in names_to in pivot_longer. This means I also have to say what those two names are separated by (which I forgot the first time, but the error message was helpful):\n\n\nlaundry %>% \n  pivot_longer(everything(), names_to=c(\"detergent\", \"temperature\"), \n               names_sep=\"_\", \n               values_to=\"dirt_removed\") -> laundry_tidy\nlaundry_tidy\n\n\n# A tibble: 24 × 3\n   detergent temperature dirt_removed\n   <chr>     <chr>              <dbl>\n 1 Super     Cold                   4\n 2 Super     Warm                   7\n 3 Super     Hot                   10\n 4 Best      Cold                   6\n 5 Best      Warm                  13\n 6 Best      Hot                   12\n 7 Super     Cold                   5\n 8 Super     Warm                   9\n 9 Super     Hot                   12\n10 Best      Cold                   6\n# … with 14 more rows\n\nSuccess.\nA plot\nThere are four observations per combination of detergent and temperature, so that devotees of ANOVA among you will know that we can test for a significant interaction effect between detergent and temperature on the amount of dirt removed. (That is to say, the effect of temperature on dirt removed might be different for each detergent, and we have enough data to see whether that is indeed the case “for all laundry loads”.)\nTo see whether this is likely, we can make an interaction plot: plot the mean dirt removed for each temperature, separately for each detergent, and then join the results for each temperature by lines (coloured by detergent). This can be done by first making a data frame of means using group_by and summarize, or like this:\n\n\nggplot(laundry_tidy, aes(x=fct_inorder(temperature), y=dirt_removed, colour=detergent, group=detergent)) + \n  stat_summary(fun.y=mean, geom=\"point\") +\n  stat_summary(fun.y=mean, geom=\"line\")\n\n\n\n\nCode-wise, the last two lines are a kind of funky geom_point and geom_line, except that instead of plotting the actual amounts of dirt removed, we plot the mean dirt removed each time. (The fct_inorder plots the temperatures in the sensible order that they appear in the data, rather than alphabetical order.)\nStatistically, if the two traces are more or less parallel, the two factors detergent and temperature act independently on the amount of dirt removed. But that is not the case here: a warm temperature is the best for Best detergent, while a hot temperature is best for Super detergent.2\nAs in actual website\nSo I lied to you (for the purpose of telling a story, but I hope a useful one).\nHere’s how the data were actually laid out:\nDetergent    Cold         Warm          Hot\nSuper     4,5,6,5     7,9,8,12   10,12,11,9\nBest      6,6,4,4  13,15,12,12  12,13,10,13\nLet’s see whether we can tell a different story by getting these data tidy. (I added the word Detergent to the top left cell to make our lives slightly easier.)\nFirst, this is column-aligned data, so we need read_table:\n\n\nmy_file=\"https://raw.githubusercontent.com/nxskok/nxskok.github.io/master/static/laundry.txt\"\nlaundry_2 <- read_table(my_file, col_types=cols(\n  Cold=col_character(),\n  Warm=col_character(),\n  Hot=col_character()\n))\nlaundry_2\n\n\n# A tibble: 2 × 4\n  Detergent Cold    Warm        Hot        \n  <chr>     <chr>   <chr>       <chr>      \n1 Super     4,5,6,5 7,9,8,12    10,12,11,9 \n2 Best      6,6,4,4 13,15,12,12 12,13,10,13\n\nMy first go at this turned out to treat the comma as a thousands separator (which was then dropped), so the top left cell got read as the number 4565. This use of col_types forces the columns to be text, so they get left alone.\nSo now, a standard pivot_longer to begin:\n\n\nlaundry_2 %>% pivot_longer(-Detergent, names_to=\"Temperature\", values_to=\"Dirt_removed\")\n\n\n# A tibble: 6 × 3\n  Detergent Temperature Dirt_removed\n  <chr>     <chr>       <chr>       \n1 Super     Cold        4,5,6,5     \n2 Super     Warm        7,9,8,12    \n3 Super     Hot         10,12,11,9  \n4 Best      Cold        6,6,4,4     \n5 Best      Warm        13,15,12,12 \n6 Best      Hot         12,13,10,13 \n\nWe have several values for dirt removed, separated by commas. We could use separate to create four new columns and pivot them longer as well. But there is a better way:\n\n\nlaundry_2 %>% pivot_longer(-Detergent, names_to=\"Temperature\", values_to=\"Dirt_removed\") %>% \n  separate_rows(Dirt_removed, convert=T) \n\n\n# A tibble: 24 × 3\n   Detergent Temperature Dirt_removed\n   <chr>     <chr>              <int>\n 1 Super     Cold                   4\n 2 Super     Cold                   5\n 3 Super     Cold                   6\n 4 Super     Cold                   5\n 5 Super     Warm                   7\n 6 Super     Warm                   9\n 7 Super     Warm                   8\n 8 Super     Warm                  12\n 9 Super     Hot                   10\n10 Super     Hot                   12\n# … with 14 more rows\n\nThis brings us back to where we were before. A couple of notes about separate_rows:\nit puts each separated value on a new row, and so is a combined separate and pivot_longer.\nthe default separator between values is everything non-alphanumeric except for a dot. That includes a comma, so we don’t have to do anything special.\nconvert=T says to turn the separated values into whatever they look like (here numbers).\nFrom here, we can proceed as before with plots, ANOVA or whatever.\nReferences\nData from here\nAlison Hill’s blog post\nIntroduction to readr\nDocumentation for separate_rows\n\nI don’t actually think I can here. I was thinking of .value, but that is used when the names of the columns that I’m making longer contain the names of new columns in them.↩︎\nThere are always two ways to express an interaction effect. The other one here is that the two detergents are pretty similar except at warm water temperatures, where Best is a lot better.↩︎\n",
    "preview": "posts/2021-11-08-two-header-rows-and-other-spreadsheets/two-header-rows-and-other-spreadsheets_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-11-12T20:37:50-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-07-13-un-counting/",
    "title": "Un-counting",
    "description": "Why you would want to do the opposite of counting",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2019-07-13",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nIntroduction\nYou probably know about count, which tells you how many observations you have in each group:\n\n\nd <- tribble(\n  ~g, ~y,\n  \"a\", 10,\n  \"a\", 13,\n  \"a\", 14, \n  \"a\", 14,\n  \"b\", 6,\n  \"b\", 7,\n  \"b\", 9\n)\n\n\n\nThere are four observations in group a and three in group b:\n\n\nd %>% count(g) -> counts\ncounts\n\n\n# A tibble: 2 × 2\n  g         n\n  <chr> <int>\n1 a         4\n2 b         3\n\nI didn’t know about this until fairly recently. Until then, I thought you had to do this:\n\n\nd %>% group_by(g) %>% \n  summarize(count=n()) \n\n\n# A tibble: 2 × 2\n  g     count\n  <chr> <int>\n1 a         4\n2 b         3\n\nwhich works, but is a lot more typing.\nGoing the other way\nThe other day, I had the opposite problem. I had a table of frequencies, and I wanted to get it back to one row per observation (I was fitting a model in Stan, and I didn’t know how to deal with frequencies). I had no idea how you might do that (without something ugly like loops), and I was almost embarrassed to stumble upon this:\n\n\ncounts %>% uncount(n)\n\n\n# A tibble: 7 × 1\n  g    \n  <chr>\n1 a    \n2 a    \n3 a    \n4 a    \n5 b    \n6 b    \n7 b    \n\nMy situation was a bit less trivial than that. I had disease category counts of coal miners with different exposures to coal dust:\n\n\nmy_url=\"https://www.utsc.utoronto.ca/~butler/d29/miners-tab.txt\"\nminers0 <- read_table(my_url)\nminers0\n\n\n# A tibble: 8 × 4\n  Exposure  None Moderate Severe\n     <dbl> <dbl>    <dbl>  <dbl>\n1      5.8    98        0      0\n2     15      51        2      1\n3     21.5    34        6      3\n4     27.5    35        5      8\n5     33.5    32       10      9\n6     39.5    23        7      8\n7     46      12        6     10\n8     51.5     4        2      5\n\nThis needs tidying to get the frequencies all into one column:\n\n\nminers0 %>% \n  gather(disease, freq, -Exposure) -> miners\nminers\n\n\n# A tibble: 24 × 3\n   Exposure disease   freq\n      <dbl> <chr>    <dbl>\n 1      5.8 None        98\n 2     15   None        51\n 3     21.5 None        34\n 4     27.5 None        35\n 5     33.5 None        32\n 6     39.5 None        23\n 7     46   None        12\n 8     51.5 None         4\n 9      5.8 Moderate     0\n10     15   Moderate     2\n# … with 14 more rows\n\nSo I wanted to fit an ordered logistic regression in Stan, predicting disease category from exposure, but I didn’t know how to handle the frequencies. If I had one row per miner, I thought…\n\n\nminers %>% uncount(freq) %>% rmarkdown::paged_table()\n\n\n\n\n{\"columns\":[{\"label\":[\"Exposure\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"disease\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"5.8\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"21.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"27.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"33.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"39.5\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"46.0\",\"2\":\"None\"},{\"1\":\"51.5\",\"2\":\"None\"},{\"1\":\"51.5\",\"2\":\"None\"},{\"1\":\"51.5\",\"2\":\"None\"},{\"1\":\"51.5\",\"2\":\"None\"},{\"1\":\"15.0\",\"2\":\"Moderate\"},{\"1\":\"15.0\",\"2\":\"Moderate\"},{\"1\":\"21.5\",\"2\":\"Moderate\"},{\"1\":\"21.5\",\"2\":\"Moderate\"},{\"1\":\"21.5\",\"2\":\"Moderate\"},{\"1\":\"21.5\",\"2\":\"Moderate\"},{\"1\":\"21.5\",\"2\":\"Moderate\"},{\"1\":\"21.5\",\"2\":\"Moderate\"},{\"1\":\"27.5\",\"2\":\"Moderate\"},{\"1\":\"27.5\",\"2\":\"Moderate\"},{\"1\":\"27.5\",\"2\":\"Moderate\"},{\"1\":\"27.5\",\"2\":\"Moderate\"},{\"1\":\"27.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"33.5\",\"2\":\"Moderate\"},{\"1\":\"39.5\",\"2\":\"Moderate\"},{\"1\":\"39.5\",\"2\":\"Moderate\"},{\"1\":\"39.5\",\"2\":\"Moderate\"},{\"1\":\"39.5\",\"2\":\"Moderate\"},{\"1\":\"39.5\",\"2\":\"Moderate\"},{\"1\":\"39.5\",\"2\":\"Moderate\"},{\"1\":\"39.5\",\"2\":\"Moderate\"},{\"1\":\"46.0\",\"2\":\"Moderate\"},{\"1\":\"46.0\",\"2\":\"Moderate\"},{\"1\":\"46.0\",\"2\":\"Moderate\"},{\"1\":\"46.0\",\"2\":\"Moderate\"},{\"1\":\"46.0\",\"2\":\"Moderate\"},{\"1\":\"46.0\",\"2\":\"Moderate\"},{\"1\":\"51.5\",\"2\":\"Moderate\"},{\"1\":\"51.5\",\"2\":\"Moderate\"},{\"1\":\"15.0\",\"2\":\"Severe\"},{\"1\":\"21.5\",\"2\":\"Severe\"},{\"1\":\"21.5\",\"2\":\"Severe\"},{\"1\":\"21.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"27.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"33.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"39.5\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"46.0\",\"2\":\"Severe\"},{\"1\":\"51.5\",\"2\":\"Severe\"},{\"1\":\"51.5\",\"2\":\"Severe\"},{\"1\":\"51.5\",\"2\":\"Severe\"},{\"1\":\"51.5\",\"2\":\"Severe\"},{\"1\":\"51.5\",\"2\":\"Severe\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\n… and so I do. (I scrolled down to check, and eventually got past the 98 miners with 5.8 years of exposure and no disease).\nFrom there, you can use this to fit the model, though I would rather have weakly informative priors for their beta and c. c is tricky, since it is ordered, but I used the idea here (near the bottom) and it worked smoothly.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T21:08:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-12-some-things-i-learned-today/",
    "title": "Some things I learned today",
    "description": "Stan files; R Markdown figures in LaTeX; Beamer in R Markdown; Build All and makefiles",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blog"
      }
    ],
    "date": "2019-05-22",
    "categories": [],
    "contents": "\nToday (or, at least, recently) I learned:\nStan\nYou can open up a .stan file of Stan code in R Studio, and the syntax gets nicely highlighted for you. Also, there is a Check button, which checks that your file is syntactically correct before you try to compile it.\nIn addition, I finally got my head around (fixed effects) ANOVA Bayesian-style, but that is a blog post for another time. (From a Bayesian point of view, random effects is a lot more natural, but my students won’t have seen that yet.)\nR Markdown figures in LaTeX\nYihui Xie is a genius (well, that much we already knew). I was generating a Beamer presentation from R Markdown, and the plots came out a bit fuzzy. I learned from The Man Himself in the first answer here that this is because knitr turns plots into .png images by default, and LaTeX doesn’t play nicely with those, especially if you want to resize them (which I did, because text-plus-plot on a slide makes the plot run off the bottom). The solution is to get knitr to generate .pdf plots, which you can do by putting a line like this in your setup chunk:\nknitr::opts_chunk$set(dev = 'pdf')\nNow my plots all look as if they have just come out of the washing machine.\nI had previously thought that I would have to keep using LaTeX to keep using Beamer, but not so: the aforementioned genius has added Beamer support to knitr. What you do is to add something like this:\noutput: \n  beamer_presentation:\n    latex_engine: lualatex\n    slide_level: 2\n    df_print: kable\n    theme: \"AnnArbor\"\n    colortheme: \"dove\"\nto your YAML front matter, tweaking the options to your preference. (I don’t seem to have decided whether I am using lualatex or xelatex: see below. I don’t want to change anything because it is working.)\nBuild All\nMy lecture notes that I am working on (actually for two courses at once, but that is another story) are R Markdown files that have other R Markdown files as child documents, and the whole thing then gets run through LaTeX to produce a Beamer presentation. Knitting the root document (usually) works, but I wanted a bit more control over it, so I wrote a Makefile to automate the process. The Makefile looks something like this:\nall: slides.pdf\n    evince slides.pdf&\nslides.md: slides.Rmd <lots of other Rmd files>\n    Rscript -e 'knitr::knit(\"slides.Rmd\")'\nslides.tex: slides.md \n    /usr/lib/rstudio/bin/pandoc/pandoc +RTS -K512m -RTS slides_c32.md --to beamer --from markdown+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash\\\n            --output slides.tex --slide-level 2 --variable theme=AnnArbor --variable colortheme=dove --highlight-style tango\\\n            --pdf-engine xelatex --self-contained \nslides.pdf: slides.tex\n    xelatex slides.tex\nThe pandoc line was shamelessly cribbed from what appears when you click the Knit button.\nAnyway, the reason for telling you this is that if you have a Makefile with an all target in it, the Build pane top right will contain a clickable Build All, and that will run your Makefile and build the All target. Likewise, there is a clickable More that will make the clean target if you happen to have one. I discovered this by accident, having discovered that I had a Build All in one of my projects, and I clicked it to see what it did. After a bit of rootling around, I found out (a) that this is what it was for and (b) I did indeed have a Makefile knocking around, that previously I had only ever run from the command line.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T21:21:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-19-changing-a-lot-of-things-in-a-lot-of-places/",
    "title": "Changing a lot of things in a lot of places",
    "description": "Making a lot of changes in text, all in one go",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2019-05-12",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\n\n\n\nIntroduction\nLet’s suppose you have a data frame like this:\n\n\n\n\n\nd\n\n\n# A tibble: 5 × 3\n  x1       x2    y     \n  <chr>    <chr> <chr> \n1 one      two   two   \n2 four     three four  \n3 seven    nine  eight \n4 six      eight seven \n5 fourteen nine  twelve\n\nWhat you want to do is to change all the even numbers in columns x1 and x2, but not y, to the number versions of themselves, so that, for example, eight becomes 8. This would seem to be a job for str_replace_all, but how to manage the multitude of changes?\nMaking a lot of changes with str_replace_all\nI learned today that you can feed str_replace_all a named vector. Wossat, you say? Well, one of these:\n\n\nquantile(1:7)\n\n\n  0%  25%  50%  75% 100% \n 1.0  2.5  4.0  5.5  7.0 \n\nThe numbers are here the five-number summary; the things next to them, that say which percentile they are, are the names attribute. You can make one of these yourself like this:\n\n\nx <- 1:3\nx\n\n\n[1] 1 2 3\n\nnames(x) <- c(\"first\", \"second\", \"third\")\nx\n\n\n first second  third \n     1      2      3 \n\nThe value of this for us is that you can feed the boatload of potential changes into str_replace_all by feeding it a named vector of the changes it might make.\nIn our example, we wanted to replace the even numbers by the numeric versions of themselves, so let’s make a little data frame with all of those:\n\n\nchanges <- tribble(\n  ~from, ~to,\n  \"two\", \"2\",\n  \"four\", \"4\",\n  \"six\", \"6\",\n  \"eight\", \"8\",\n  \"ten\", \"10\",\n  \"twelve\", \"12\",\n  \"fourteen\", \"14\"\n)\n\n\n\nI think this is as high as we need to go. I like a tribble for this so that you can easily see what is going to replace what.\nFor the named vector, the values are the new values (the ones I called to in changes), while the names are the old ones (from). So let’s construct that. There is one extra thing: I want to replace whole words only (and not end up with something like 4teen, which sounds like one of those 90s boy bands), so what I’ll do is to put “word boundaries”1 around the from values:2\n\n\nmy_changes <- changes$to\nnames(my_changes) <- str_c(\"\\\\b\", changes$from, \"\\\\b\")\nmy_changes\n\n\n     \\\\btwo\\\\b     \\\\bfour\\\\b      \\\\bsix\\\\b    \\\\beight\\\\b \n           \"2\"            \"4\"            \"6\"            \"8\" \n     \\\\bten\\\\b   \\\\btwelve\\\\b \\\\bfourteen\\\\b \n          \"10\"           \"12\"           \"14\" \n\nand that seems to reflect the changes we want to make. So let’s make it go, just on columns x1 and x2:3\n\n\nd %>% mutate_at(\n  vars(starts_with(\"x\")),\n       ~ str_replace_all(., my_changes)\n  )\n\n\n# A tibble: 5 × 3\n  x1    x2    y     \n  <chr> <chr> <chr> \n1 one   2     two   \n2 4     three four  \n3 seven nine  eight \n4 6     8     seven \n5 14    nine  twelve\n\n“for each of the columns that starts with x, replace everything in it that’s in the recipe in my_changes.”\nIt seems to have worked, and not a 90s boy band in sight.\n\nThis Stack Overflow answer explains why the backslashes need to be doubled. The answer is for Python, but the same issue applies to R.↩︎\nThis means that the number names only match if they are surrounded by non-word characters, that is, spaces, or the beginning or end of the text.↩︎\nThe modern way to do this is to use across, but I wrote this post in 2019, and this is all we had then.↩︎\n",
    "preview": {},
    "last_modified": "2021-11-19T22:16:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-19-distance-between-clusters/",
    "title": "Distance between clusters",
    "description": "How far apart are two *clusters* of objects, when all I have are distances between objects?",
    "author": [
      {
        "name": "Ken Butler",
        "url": "http://ritsokiguess.site/blogg"
      }
    ],
    "date": "2019-04-23",
    "categories": [],
    "contents": "\nPackages\n\n\nlibrary(tidyverse)\nlibrary(spatstat.geom)\n\n\n\nIntroduction\nHierarchical cluster analysis is based on distances between individuals. This can be defined via Euclidean distance, Manhattan distance, a matching coefficient, etc. I won’t get into these further.\nThere is a second problem, though, which I do wish to discuss: when you carry out a hierarchical cluster analysis, you want to join the two closest clusters together at each step. But how do you work out how far apart two clusters are, when all you have are distances between individuals? Here, I give some examples, and, perhaps more interesting, some visuals with the code that makes them.\nInter-cluster distances\nSome clusters to find differences between\nLet’s make some random points in two dimensions that are in two clusters: 5 points each, uniformly distributed on \\((0,20)^2\\) and \\((20,40)^2\\):\n\n\nset.seed(457299)\nA=tibble(x=runif(5,0,20),y=runif(5,0,20))\nB=tibble(x=runif(5,20,40),y=runif(5,20,40))\nddd=bind_rows(A=A,B=B,.id=\"cluster\")\ng=ggplot(ddd,aes(x=x,y=y,colour=cluster))+geom_point()+\n  coord_fixed(xlim=c(0,40),ylim=c(0,40))\ng\n\n\n\n\nNote that I gave this graph a name, so that I can add things to it later.\nWe know how to measure distances between individuals, but what about distances between clusters?\nSingle linkage\nOne way to measure the distance between two clusters is to pick a point from each cluster to represent that cluster, and use the distance between those. For example, we might find the points in cluster A and and in cluster B that are closest together, and say that the distance between the two clusters is the distance between those two points.\nSo, we need all the distances between a point in A and a point in B. The package spatstat.geom has a function crossdist that does exactly this:\n\n\ndist=distances=spatstat.geom::crossdist(A$x, A$y, B$x, B$y)\ndist\n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 25.15504 17.21720 12.36243 24.28935 23.34414\n[2,] 43.98827 35.44714 29.91026 41.73130 42.46700\n[3,] 42.82409 34.39974 28.93598 40.86692 41.19940\n[4,] 32.97612 24.52382 19.07809 31.05325 31.42569\n[5,] 32.54432 23.73306 18.08921 29.39528 31.48468\n\nThis is a matrix. From this, we want to find out which points are the two that have the smallest distance between them. It looks like point 1 in A and point 3 in B (the middle distance of the top row). We can use base R to verify this:\n\n\nwm1=which.min(apply(distances,1,min))\nwm1\n\n\n[1] 1\n\nwm2=which.min(apply(distances,2,min))\nwm2\n\n\n[1] 3\n\nclosest=bind_rows(A=A[wm1,],B=B[wm2,],.id=\"cluster\")\nclosest\n\n\n# A tibble: 2 × 3\n  cluster     x     y\n  <chr>   <dbl> <dbl>\n1 A        19.0  15.0\n2 B        22.8  26.8\n\nSo we can join the two closest points, now that we know where they are:\n\n\ng+geom_segment(data=closest,aes(x=x[1],y=y[1],xend=x[2],yend=y[2]),colour=\"darkgreen\")\n\n\n\n\nThis works, but it isn’t very elegant (or very tidyverse).\nI usually use crossing for this kind of thing, but the points in A and B have both an x and a y coordinate. I use a hack: unite to combine them together into one thing, then separate after making all possible pairs:\n\n\nA %>% unite(coord, x, y) -> a1\nB %>% unite(coord, x, y) -> b1\ncrossing(A_coord=a1$coord, B_coord=b1$coord) %>% \n  separate(A_coord, into=c(\"A_x\", \"A_y\"), sep=\"_\", convert=T) %>% \n  separate(B_coord, into=c(\"B_x\", \"B_y\"), sep=\"_\", convert=T) \n\n\n# A tibble: 25 × 4\n     A_x   A_y   B_x   B_y\n   <dbl> <dbl> <dbl> <dbl>\n 1  11.2  11.7  22.8  26.8\n 2  11.2  11.7  27.4  30.0\n 3  11.2  11.7  28.8  37.3\n 4  11.2  11.7  35.2  34.2\n 5  11.2  11.7  35.7  31.3\n 6  19.0  15.0  22.8  26.8\n 7  19.0  15.0  27.4  30.0\n 8  19.0  15.0  28.8  37.3\n 9  19.0  15.0  35.2  34.2\n10  19.0  15.0  35.7  31.3\n# … with 15 more rows\n\nThe reason for the sep in the separate is that separate also counts the decimal points as separators, which I want to exclude; the only separators should be the underscores that unite introduced. The convert turns the coordinates back into numbers.\nNow I find the (Euclidean) distances and then the smallest one:\n\n\ncrossing(A_coord=a1$coord, B_coord=b1$coord) %>% \n  separate(A_coord, into=c(\"A_x\", \"A_y\"), sep=\"_\", convert=T) %>% \n  separate(B_coord, into=c(\"B_x\", \"B_y\"), sep=\"_\", convert=T) %>% \n  mutate(dist=sqrt((A_x-B_x)^2+(A_y-B_y)^2)) -> distances\ndistances %>% arrange(dist) %>% slice(1) -> d\nd\n\n\n# A tibble: 1 × 5\n    A_x   A_y   B_x   B_y  dist\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  19.0  15.0  22.8  26.8  12.4\n\nand then I add it to my plot:\n\n\ng+geom_segment(data=d, aes(x=A_x, y=A_y, xend=B_x, yend=B_y), colour=\"darkgreen\")\n\n\n\n\nA problem with single linkage is that two clusters are close if a point in A and a point in B happen to be close. The other red and blue points on the graph could be anywhere. You could say that this goes against two clusters being “close”. The impact in cluster analysis is that you get “stringy” clusters where single points are added on to clusters one at a time. Can we improve on that?\nComplete linkage\nAnother way to measure the distance between two clusters is the longest distance between a point in A and a point in B. This will make two clusters close if everything in the two clusters is close. You could reasonably argue that this is a better idea than single linkage.\nAfter the work we did above, this is simple to draw: take the data frame distances, find the largest distance, and add it to the plot:\n\n\ndistances %>% arrange(desc(dist)) %>% slice(1) -> d\ng+geom_segment(data=d, aes(x=A_x, y=A_y, xend=B_x, yend=B_y), colour=\"darkgreen\")\n\n\n\n\nWard’s method\nLet’s cast our minds back to analysis of variance, which gives another way of thinking about distance between groups (in one dimension). Consider these data:\n\n\nd1=tribble(\n  ~y, ~g,\n  10, \"A\",\n  11, \"A\",\n  13, \"A\",\n  11, \"B\",\n  12, \"B\",\n  14, \"B\"\n)\nd1\n\n\n# A tibble: 6 × 2\n      y g    \n  <dbl> <chr>\n1    10 A    \n2    11 A    \n3    13 A    \n4    11 B    \n5    12 B    \n6    14 B    \n\nThe two groups here are pretty close together, relative to how spread out they are:\n\n\nggplot(d1, aes(x=g, y=y, colour=g))+geom_point()\n\n\n\n\nand the analysis of variance concurs:\n\n\nd1.1=aov(y~g, data=d1)\nsummary(d1.1)\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)\ng            1  1.500   1.500   0.643  0.468\nResiduals    4  9.333   2.333               \n\nThe \\(F\\)-value is small because the variability between groups is small compared to the variability within groups; it’s reasonable to act as if the two groups have the same mean.\nCompare that with this data set:\n\n\nd2=tribble(\n  ~y, ~g,\n  10, \"A\",\n  11, \"A\",\n  13, \"A\",\n  21, \"B\",\n  22, \"B\",\n  24, \"B\"\n)\nd2\n\n\n# A tibble: 6 × 2\n      y g    \n  <dbl> <chr>\n1    10 A    \n2    11 A    \n3    13 A    \n4    21 B    \n5    22 B    \n6    24 B    \n\n\n\nggplot(d2, aes(x=g, y=y, colour=g))+geom_point()\n\n\n\n\nHow do within-groups and between-groups compare this time?\n\n\nd2.1=aov(y~g, data=d2)\nsummary(d2.1)\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ng            1 181.50  181.50   77.79 0.000912 ***\nResiduals    4   9.33    2.33                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis time the variability between groups is much larger than the variability within groups; we have (strong) evidence that the group means are different, and it makes sense to treat the groups separately.\nHow does that apply to cluster distances? Well, what is happening here is comparing squared distances from group means to distances from the overall mean. Let’s see:\n\n\nd1 %>% summarize(m=mean(y)) -> d1.overall\nd1.overall\n\n\n# A tibble: 1 × 1\n      m\n  <dbl>\n1  11.8\n\nd1 %>% mutate(mean=d1.overall$m) %>% \n  mutate(diffsq=(y-mean)^2) %>% \n  summarize(total=sum(diffsq))\n\n\n# A tibble: 1 × 1\n  total\n  <dbl>\n1  10.8\n\nThis is the sum of squared differences from the mean of all the observations taken together. What about the same thing, but from each group mean?\n\n\nd1 %>% group_by(g) %>% summarize(m=mean(y)) -> d1.groups\nd1.groups\n\n\n# A tibble: 2 × 2\n  g         m\n  <chr> <dbl>\n1 A      11.3\n2 B      12.3\n\nd1 %>% left_join(d1.groups) %>% \n  mutate(diffsq=(y-m)^2) %>% \n  summarize(total=sum(diffsq))\n\n\n# A tibble: 1 × 1\n  total\n  <dbl>\n1  9.33\n\nOne way to measure the distance between two groups (clusters) is to take the difference of these. The observations will always be closer to their own group mean than to the combined mean, but in this case the difference is small:\n\n\n10.8-9.33\n\n\n[1] 1.47\n\nThinking of these as clusters, these are close together and could easily be combined.\nWhat about the two groups that look more distinct?\nThe distance from the overall mean is:\n\n\nd2 %>% summarize(m=mean(y)) -> d2.overall\nd2.overall\n\n\n# A tibble: 1 × 1\n      m\n  <dbl>\n1  16.8\n\nd2 %>% mutate(mean=d2.overall$m) %>% \n  mutate(diffsq=(y-mean)^2) %>% \n  summarize(total=sum(diffsq))\n\n\n# A tibble: 1 × 1\n  total\n  <dbl>\n1  191.\n\nand from the separate group means is\n\n\nd2 %>% group_by(g) %>% summarize(m=mean(y)) -> d2.groups\nd2.groups\n\n\n# A tibble: 2 × 2\n  g         m\n  <chr> <dbl>\n1 A      11.3\n2 B      22.3\n\nd2 %>% left_join(d2.groups) %>% \n  mutate(diffsq=(y-m)^2) %>% \n  summarize(total=sum(diffsq))\n\n\n# A tibble: 1 × 1\n  total\n  <dbl>\n1  9.33\n\nThis time the difference is\n\n\n191-9.33\n\n\n[1] 181.67\n\nThis is much bigger, and combining these groups would not be a good idea.\nFor cluster analysis, these ideas are behind Ward’s method. Compare the distances of each point from the mean of the clusters they currently belong to, with the distances from the mean of those two clusters combined. If the difference between these is small, the two clusters could be combined; if not, the two clusters should not be combined if possible.\nHow does this look on a picture? I did this in my lecture notes with some hairy for-loops, but I think I can do better.\nLet’s first work out the means for each of x and y for my clusters:\n\n\nddd %>% group_by(cluster) %>% \n  summarize(mean_x=mean(x), mean_y=mean(y)) -> means\nmeans\n\n\n# A tibble: 2 × 3\n  cluster mean_x mean_y\n  <chr>    <dbl>  <dbl>\n1 A         9.01   10.5\n2 B        30.0    31.9\n\nNow I look up which cluster each observation was in and add its mean. (I surreptitiously used this idea above):\n\n\nddd %>% left_join(means) -> group_means\ngroup_means\n\n\n# A tibble: 10 × 5\n   cluster     x     y mean_x mean_y\n   <chr>   <dbl> <dbl>  <dbl>  <dbl>\n 1 A       19.0  15.0    9.01   10.5\n 2 A        2.49  4.84   9.01   10.5\n 3 A        4.55  4.34   9.01   10.5\n 4 A       11.2  11.7    9.01   10.5\n 5 A        7.88 16.6    9.01   10.5\n 6 B       35.2  34.2   30.0    31.9\n 7 B       27.4  30.0   30.0    31.9\n 8 B       22.8  26.8   30.0    31.9\n 9 B       28.8  37.3   30.0    31.9\n10 B       35.7  31.3   30.0    31.9\n\nand then I think I can add the lines, coloured by cluster, thus:\n\n\ng+geom_segment(data=group_means, aes(x=x, y=y, xend=mean_x, yend=mean_y, colour=cluster))\n\n\n\n\nThe points are reasonably close to their group means.\nHow does that compare to the distances from the overall mean? First we have to work that out:\n\n\nddd %>% summarize(mean_x=mean(x), mean_y=mean(y)) -> means\n\n\n\nThen we glue this onto to the original points:\n\n\nddd %>% mutate(mean_x=means$mean_x, mean_y=means$mean_y) -> overall_means\noverall_means\n\n\n# A tibble: 10 × 5\n   cluster     x     y mean_x mean_y\n   <chr>   <dbl> <dbl>  <dbl>  <dbl>\n 1 A       19.0  15.0    19.5   21.2\n 2 A        2.49  4.84   19.5   21.2\n 3 A        4.55  4.34   19.5   21.2\n 4 A       11.2  11.7    19.5   21.2\n 5 A        7.88 16.6    19.5   21.2\n 6 B       35.2  34.2    19.5   21.2\n 7 B       27.4  30.0    19.5   21.2\n 8 B       22.8  26.8    19.5   21.2\n 9 B       28.8  37.3    19.5   21.2\n10 B       35.7  31.3    19.5   21.2\n\nand then repeat the previous idea to plot them:\n\n\ng+geom_segment(data=overall_means, aes(x=x, y=y, xend=mean_x, yend=mean_y), colour=\"darkgreen\")\n\n\n\n\nThe points are a lot further from the overall mean than from the group means (the green lines overall are longer than the red and blue ones), suggesting that the clusters are, in the sense of Ward’s method, a long way apart.\n\n\n\n",
    "preview": "posts/2021-11-19-distance-between-clusters/distance-between-clusters_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-11-19T22:16:22-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
